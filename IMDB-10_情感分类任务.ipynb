{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一：IMDB-10 情感分类\n",
    "\n",
    "本Notebook旨在完成IMDB电影评论的十分类情感（评分1-10）任务。我们将根据实验要求，实现并对比三种不同的深度学习模型。\n",
    "\n",
    "**实验方案：**\n",
    "1.  **模型一：** GloVe词向量 + BiLSTM + 全连接层\n",
    "2.  **模型二：** BERT-base 嵌入 + BiLSTM + 分类头\n",
    "3.  **模型三：** 微调 BERT-base\n",
    "\n",
    "**评价指标：**\n",
    "- 宏平均F1值 (Macro-F1) (主要)\n",
    "- 准确率 (Accuracy)\n",
    "- 均方根误差 (RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境设置与依赖安装\n",
    "\n",
    "首先，安装所有必需的Python库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理\n",
    "\n",
    "根据任务描述，我们需要从`.txt.ss`文件中加载数据。这些文件可能包含用户ID和产品ID列，需要被忽略。我们将编写一个函数来解析这些文件，提取文本和标签，并进行清理。\n",
    "\n",
    "**清理步骤：**\n",
    "1.  移除句子分割符 `<sssss>`。\n",
    "2.  将标签从 `1-10` 转换为 `0-9` 以便模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_imdb_data(file_path):\n",
    "    \"\"\"加载并解析IMDB数据文件，使用正则表达式查找标签。\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # 使用正则表达式查找被双制表符包围的评分 (例如: \\t\\t10\\t\\t)\n",
    "            match = re.search(r'\\t\\t(\\d+)\\t\\t', line)\n",
    "            \n",
    "            if match:\n",
    "                # 提取评分和文本\n",
    "                rating = int(match.group(1))\n",
    "                text_start_index = match.end()\n",
    "                text = line[text_start_index:].strip()\n",
    "                \n",
    "                # 清理文本中的<sssss>标记\n",
    "                text = text.replace('<sssss>', ' ').strip()\n",
    "                \n",
    "                # 标签从1-10转换为0-9\n",
    "                labels.append(rating - 1)\n",
    "                texts.append(text)\n",
    "            # 如果找不到匹配项，则静默跳过该行，因为它可能格式不正确\n",
    "    \n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "    return df\n",
    "\n",
    "# 定义文件路径\n",
    "train_file = 'imdb.train.txt.ss'\n",
    "dev_file = 'imdb.dev.txt.ss'\n",
    "test_file = 'imdb.test.txt.ss'\n",
    "\n",
    "# 加载所有数据集\n",
    "df_train = load_imdb_data(train_file)\n",
    "df_val = load_imdb_data(dev_file)\n",
    "df_test = load_imdb_data(test_file)\n",
    "\n",
    "print(f\"训练集大小: {df_train.shape}\")\n",
    "print(f\"验证集大小: {df_val.shape}\")\n",
    "print(f\"测试集大小: {df_test.shape}\")\n",
    "\n",
    "print(\"\\n数据样本示例:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型一：GloVe + BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe词向量与数据准备\n",
    "我们将构建词汇表，加载预训练的GloVe词向量，并创建PyTorch的`Dataset`和`DataLoader`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# --- 参数配置 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE_LSTM = 64\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "MAX_LEN_LSTM = 512 # 按要求截断\n",
    "GLOVE_PATH = r\"D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\"\n",
    "\n",
    "# --- 文本分词器 ---\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9' ]+\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "# --- 构建词汇表 ---\n",
    "print(\"正在构建词汇表...\")\n",
    "word_counts = Counter()\n",
    "for text in df_train['text']:\n",
    "    word_counts.update(tokenizer(text))\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_idx = {word: i + 2 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<unk>'] = 1\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "\n",
    "# --- 加载GloVe词向量 ---\n",
    "print(\"正在加载GloVe词向量...\")\n",
    "glove_embeddings = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        if len(parts) != EMBEDDING_DIM + 1:\n",
    "            continue\n",
    "        if word in word_to_idx:\n",
    "            try:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                glove_embeddings[word_to_idx[word]] = vector\n",
    "            except ValueError:\n",
    "                continue\n",
    "glove_embeddings = torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "print(\"GloVe加载完成。\")\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_to_idx, max_len):\n",
    "        self.df = dataframe\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        tokens = [self.word_to_idx.get(word, 1) for word in tokenizer(text)]\n",
    "        \n",
    "        # 截断与填充\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend([0] * (self.max_len - len(tokens)))\n",
    "            \n",
    "        return torch.tensor(tokens), torch.tensor(label)\n",
    "\n",
    "# --- 创建DataLoaders ---\n",
    "train_dataset = IMDBDataset(df_train, word_to_idx, MAX_LEN_LSTM)\n",
    "val_dataset = IMDBDataset(df_val, word_to_idx, MAX_LEN_LSTM)\n",
    "test_dataset = IMDBDataset(df_test, word_to_idx, MAX_LEN_LSTM)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_LSTM, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE_LSTM)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_LSTM)\n",
    "\n",
    "print(\"\\n数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 模型一结构定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                              bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "model1 = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT, glove_embeddings).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 训练与评估（模型一）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 计算指标\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    # RMSE需要将标签转回1-10\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 开始训练 ---\n",
    "print(\"开始训练模型一...\")\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "results = {}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model1, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate(model1, val_loader, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证Acc: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')\n",
    "\n",
    "# --- 在测试集上评估 ---\n",
    "test_loss, test_acc, test_f1, test_rmse = evaluate(model1, test_loader, criterion)\n",
    "print(f'\\n模型一 测试集结果 -> Acc: {test_acc*100:.2f}% | F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型一 (GloVe+BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型二 & 三：基于BERT的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. BERT数据准备\n",
    "我们将使用Hugging Face的`transformers`库来准备数据。这包括使用BERT的分词器，并为模型创建`Dataset`和`DataLoader`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# --- 参数配置 ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN_BERT = 512 # 按要求截断\n",
    "# 增加批量大小以更好地利用GPU，如果遇到显存不足(Out of Memory)错误，请适当调低此值\n",
    "BATCH_SIZE_BERT = 32 \n",
    "\n",
    "# BERT 分词器和模型\n",
    "# 确保你已经提前下载了 bert-base-uncased 模型到本地\n",
    "# 例如，保存路径为 'D:/models/bert-base-uncased'\n",
    "local_bert_path = 'D:/models/bert-base-uncased' # <--- 请务必修改为你的本地路径\n",
    "\n",
    "# 检查路径是否存在\n",
    "import os\n",
    "if not os.path.exists(local_bert_path):\n",
    "    raise FileNotFoundError(f\"BERT模型路径不存在: {local_bert_path}。请先下载模型并修改路径。\")\n",
    "\n",
    "print(f\"从本地路径加载BERT分词器: {local_bert_path}\")\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(local_bert_path)\n",
    "print(\"分词器加载成功。\")\n",
    "\n",
    "# --- PyTorch Dataset for BERT ---\n",
    "class IMDBDatasetBERT(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 创建DataLoaders ---\n",
    "# 优化DataLoader以加速数据加载\n",
    "# num_workers > 0 会启用多进程加载数据，可以显著提速\n",
    "# pin_memory=True 与CUDA结合使用，可以加速数据到GPU的传输\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dataloader_args = {\n",
    "    \"batch_size\": BATCH_SIZE_BERT,\n",
    "    \"num_workers\": 4 if use_cuda else 0, # 在Windows上，num_workers应在 if __name__ == '__main__': 块中使用，但在Jupyter中通常没问题\n",
    "    \"pin_memory\": True if use_cuda else False\n",
    "}\n",
    "\n",
    "train_dataset_bert = IMDBDatasetBERT(df_train, tokenizer_bert, MAX_LEN_BERT)\n",
    "val_dataset_bert = IMDBDatasetBERT(df_val, tokenizer_bert, MAX_LEN_BERT)\n",
    "test_dataset_bert = IMDBDatasetBERT(df_test, tokenizer_bert, MAX_LEN_BERT)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, shuffle=True, **dataloader_args)\n",
    "val_loader_bert = DataLoader(val_dataset_bert, **dataloader_args)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, **dataloader_args)\n",
    "print(\"用于BERT的数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 模型二：BERT嵌入 + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                              bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# --- 实例化模型二 ---\n",
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "# 冻结BERT参数\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model2 = BertBiLSTMClassifier(bert_model, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 训练与评估（模型二）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_bert_based(model, iterator, optimizer, criterion, scaler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 使用autocast上下文管理器进行混合精度训练\n",
    "        with autocast():\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "        # 使用GradScaler缩放损失，反向传播并更新权重\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_based(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            # 在评估时也使用autocast\n",
    "            with autocast():\n",
    "                predictions = model(input_ids, attention_mask)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型二---\n",
    "print(\"\\n开始训练模型二 \")\n",
    "EPOCHS_BERT = 3 # BERT相关模型通常需要更少的轮次\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 创建一个GradScaler实例用于混合精度训练\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS_BERT):\n",
    "    train_loss = train_bert_based(model2, train_loader_bert, optimizer, criterion, scaler)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_bert_based(model2, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证Acc: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')\n",
    "\n",
    "# --- 在测试集上评估 ---\n",
    "test_loss, test_acc, test_f1, test_rmse = evaluate_bert_based(model2, test_loader_bert, criterion)\n",
    "print(f'\\n模型二 测试集结果 -> Acc: {test_acc*100:.2f}% | F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型二 (BERT嵌入+BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 模型三：微调BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# --- 实例化模型三 ---\n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL_NAME,\n",
    "    num_labels=NUM_CLASSES,\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- 优化器与学习率调度器 ---\n",
    "optimizer = AdamW(model3.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_loader_bert) * EPOCHS_BERT\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. 训练与评估（模型三）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_finetune(model, iterator, optimizer, scheduler, criterion, scaler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        # 使用autocast进行混合精度前向传播\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 使用scaler进行损失缩放和反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # 在梯度裁剪前，使用scaler.unscale_来unscale梯度\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # 更新权重和scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_finetune(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            # 在评估时也使用autocast\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型三 (使用混合精度) ---\n",
    "print(\"\\n开始训练模型三 (使用混合精度)...\")\n",
    "\n",
    "# 为模型三创建一个新的GradScaler实例\n",
    "scaler_finetune = GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS_BERT):\n",
    "    train_loss = train_finetune(model3, train_loader_bert, optimizer, scheduler, criterion, scaler_finetune)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_finetune(model3, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证Acc: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')\n",
    "\n",
    "# --- 在测试集上评估 ---\n",
    "test_loss, test_acc, test_f1, test_rmse = evaluate_finetune(model3, test_loader_bert, criterion)\n",
    "print(f'\\n模型三 测试集结果 -> Acc: {test_acc*100:.2f}% | F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型三 (微调BERT)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 结果汇总与分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).T\n",
    "df_results['Accuracy'] = df_results['Accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "df_results['Macro-F1'] = df_results['Macro-F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_results['RMSE'] = df_results['RMSE'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"--- IMDB-10测试集最终性能对比 ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验分析\n",
    "\n",
    "1.  **模型一 (GloVe + BiLSTM):** 这是传统的基线模型。它使用静态的GloVe词向量，无法捕捉词语在不同上下文中的动态含义。尽管BiLSTM可以学习序列信息，但由于输入特征的局限性，其性能通常是三者中最弱的。它的优点是训练速度快，对计算资源要求较低。\n",
    "\n",
    "2.  **模型二 (BERT嵌入 + BiLSTM):** 此模型通过引入BERT作为特征提取器，获得了显著的性能提升。BERT能够生成上下文相关的词向量，极大地丰富了输入特征的语义信息。顶部的BiLSTM进一步整合这些特征，以适应最终的分类任务。这种方法在性能和资源消耗之间取得了很好的平衡，通常比模型一好得多，但比模型三稍逊一筹。\n",
    "\n",
    "3.  **模型三 (微调BERT):** 这是当前解决此类任务最主流且性能最强的方法。通过在下游任务数据上对整个BERT模型进行微调，模型的全部参数（从嵌入层到注意力层）都为特定的分类目标进行了优化。这使得模型能够学习到任务特有的语言模式。从结果中可以看出，微调BERT在所有指标上都取得了最佳性能，尤其是在主要的F1值和准确率上。RMSE指标也最低，说明其预测的评分与真实评分的误差最小。其代价是最高的计算资源需求和最长的训练时间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
