{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一：IMDB-10 情感分类（基于SNLI模板修改）\n",
    "\n",
    "本Notebook为IMDB电影评论的十分类情感（评分1-10）任务，基于SNLI代码模板修改而来。\n",
    "\n",
    "三个模型分别是：\n",
    "1.  **模型一：** GloVe嵌入 + BiLSTM\n",
    "2.  **模型二：** BERT-base嵌入 + BiLSTM (作为特征提取器)\n",
    "3.  **模型三：** 微调BERT-base模型\n",
    "\n",
    "**评价指标：** 准确率（Accuracy）、宏平均F1值（Macro-F1）和均方根误差（RMSE）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境设置与依赖安装\n",
    "\n",
    "首先，我们安装必要的库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理\n",
    "\n",
    "我们将从 `.txt.ss` 文件加载IMDB数据集。数据集包含电影评论文本和评分标签。\n",
    "\n",
    "**标签说明：**\n",
    "- `0-9`: 评分1-10转换后的标签\n",
    "\n",
    "我们需要将评分从1-10转换为0-9以便模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting pandas\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/01/a5/931fc3ad333d9d87b10107d948d757d67ebcfc33b1988d5faccc39c6845c/pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.26.0 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: (67426, 2)\n",
      "验证集大小: (8381, 2)\n",
      "测试集大小: (9112, 2)\n",
      "\n",
      "数据样本示例:\n",
      "                                                text  label\n",
      "0  i excepted a lot from this movie , and it did ...      9\n",
      "1  this movie is not worth seeing .   has no meri...      0\n",
      "2  this is a truly remarkable horror movie .   al...      9\n",
      "3  * minor spoilers * this movie is inept .   so ...      2\n",
      "4  this is a brilliant horror movie .   fans of t...      9\n",
      "\n",
      "训练集标签分布:\n",
      "label\n",
      "0     1838\n",
      "1     1589\n",
      "2     2359\n",
      "3     3499\n",
      "4     5653\n",
      "5     8666\n",
      "6    12849\n",
      "7    13673\n",
      "8     8330\n",
      "9     8970\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_imdb_data(file_path):\n",
    "    \"\"\"加载并解析IMDB数据文件，使用正则表达式查找标签。\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                match = re.search(r'\\t\\t(\\d+)\\t\\t', line)\n",
    "                if match:\n",
    "                    rating = int(match.group(1))\n",
    "                    text_start_index = match.end()\n",
    "                    text = line[text_start_index:].strip()\n",
    "                    text = text.replace('<sssss>', ' ').strip()\n",
    "                    labels.append(rating - 1)  # 转换为0-9\n",
    "                    texts.append(text)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {file_path}。请确保数据文件在当前目录下。\")\n",
    "        return pd.DataFrame({'text': [], 'label': []})\n",
    "    \n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# 定义文件路径\n",
    "train_file = 'imdb.train.txt.ss'\n",
    "dev_file = 'imdb.dev.txt.ss'\n",
    "test_file = 'imdb.test.txt.ss'\n",
    "\n",
    "# 加载所有数据集\n",
    "df_train = load_imdb_data(train_file)\n",
    "df_val = load_imdb_data(dev_file)\n",
    "df_test = load_imdb_data(test_file)\n",
    "\n",
    "if not df_train.empty:\n",
    "    print(f\"训练集大小: {df_train.shape}\")\n",
    "    print(f\"验证集大小: {df_val.shape}\")\n",
    "    print(f\"测试集大小: {df_test.shape}\")\n",
    "    print(\"\\n数据样本示例:\")\n",
    "    print(df_train.head())\n",
    "    \n",
    "    # 显示标签分布\n",
    "    print(\"\\n训练集标签分布:\")\n",
    "    print(df_train['label'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"数据加载失败，请检查文件路径！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型一：GloVe + BiLSTM\n",
    "\n",
    "该模型使用预训练的GloVe词向量作为BiLSTM网络的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe设置\n",
    "使用300维的向量。可以选择使用glove.6B.300d.txt（较小）或glove.840B.300d.txt（较大）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /root/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /root/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /root/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "警告: GloVe文件不存在于: glove.6B.300d.txt\n",
      "将使用随机初始化的词向量\n",
      "正在构建词汇表...\n",
      "词汇表大小: 86008\n",
      "正在加载GloVe词向量...\n",
      "使用随机初始化的词向量\n",
      "\n",
      "数据准备完成。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 参数配置 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10  # IMDB 10分类\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "MAX_LEN = 256  # 算力原因调成256\n",
    "\n",
    "# GloVe路径 \n",
    "GLOVE_PATH = r\"glove.6B.300d.txt\"  \n",
    "\n",
    "\n",
    "# 检查GloVe文件是否存在\n",
    "if not os.path.exists(GLOVE_PATH):\n",
    "    print(f\"警告: GloVe文件不存在于: {GLOVE_PATH}\")\n",
    "    print(\"将使用随机初始化的词向量\")\n",
    "    USE_GLOVE = False\n",
    "else:\n",
    "    print(f\"找到GloVe文件: {GLOVE_PATH}\")\n",
    "    USE_GLOVE = True\n",
    "\n",
    "# --- 文本预处理与词汇表构建 ---\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9' ]+\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "print(\"正在构建词汇表...\")\n",
    "word_counts = Counter()\n",
    "for text in df_train['text']:\n",
    "    word_counts.update(tokenizer(text))\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_idx = {word: i+2 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<unk>'] = 1\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "\n",
    "print(f\"词汇表大小: {VOCAB_SIZE}\")\n",
    "\n",
    "# --- GloVe 词向量矩阵 ---\n",
    "print(\"正在加载GloVe词向量...\")\n",
    "glove_embeddings = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "word_found = 0\n",
    "\n",
    "if USE_GLOVE:\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line_num % 50000 == 0:\n",
    "                print(f\"已处理 {line_num} 行...\")\n",
    "            \n",
    "            try:\n",
    "                parts = line.strip().split(' ')\n",
    "                word = parts[0]\n",
    "                \n",
    "                if word in word_to_idx:\n",
    "                    if len(parts) >= EMBEDDING_DIM + 1:\n",
    "                        vector = np.array(parts[1:EMBEDDING_DIM+1], dtype=np.float32)\n",
    "                        if len(vector) == EMBEDDING_DIM:\n",
    "                            glove_embeddings[word_to_idx[word]] = vector\n",
    "                            word_found += 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"成功加载 {word_found} 个词向量 ({word_found/VOCAB_SIZE*100:.2f}% 的词汇表)\")\n",
    "else:\n",
    "    print(\"使用随机初始化的词向量\")\n",
    "    glove_embeddings = np.random.normal(0, 0.1, (VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "glove_embeddings = torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "\n",
    "# --- PyTorch 数据集 ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_to_idx, max_len):\n",
    "        self.df = dataframe\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        tokens = [self.word_to_idx.get(word, self.word_to_idx['<unk>']) for word in tokenizer(text)]\n",
    "        \n",
    "        # 填充/截断\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens.extend([self.word_to_idx['<pad>']] * (self.max_len - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(tokens), torch.tensor(label)\n",
    "\n",
    "# --- 创建数据加载器 ---\n",
    "train_dataset = IMDBDataset(df_train, word_to_idx, MAX_LEN)\n",
    "val_dataset = IMDBDataset(df_val, word_to_idx, MAX_LEN)\n",
    "test_dataset = IMDBDataset(df_test, word_to_idx, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\n数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BiLSTM 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (embedding): Embedding(86008, 300, padding_idx=0)\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False  # 冻结词向量\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                              hidden_dim, \n",
    "                              num_layers=n_layers, \n",
    "                              bidirectional=True, \n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2因为是双向LSTM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 拼接前向和后向的最终隐藏状态\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 实例化模型\n",
    "model1 = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT, glove_embeddings).to(DEVICE)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 训练与评估循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型一 (GloVe + BiLSTM)...\n",
      "轮次: 01 | 训练损失: 2.100 | 训练准确率: 20.04% | 验证损失: 2.086 | 验证准确率: 20.73% | 验证F1: 0.034 | 验证RMSE: 2.437\n",
      "轮次: 02 | 训练损失: 2.098 | 训练准确率: 19.99% | 验证损失: 2.086 | 验证准确率: 20.70% | 验证F1: 0.035 | 验证RMSE: 2.437\n",
      "轮次: 03 | 训练损失: 2.095 | 训练准确率: 20.10% | 验证损失: 2.081 | 验证准确率: 20.73% | 验证F1: 0.034 | 验证RMSE: 2.437\n",
      "轮次: 04 | 训练损失: 2.096 | 训练准确率: 19.97% | 验证损失: 2.087 | 验证准确率: 20.76% | 验证F1: 0.035 | 验证RMSE: 2.436\n",
      "轮次: 05 | 训练损失: 2.095 | 训练准确率: 20.11% | 验证损失: 2.078 | 验证准确率: 20.73% | 验证F1: 0.034 | 验证RMSE: 2.437\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = accuracy_score(labels.cpu(), predictions.argmax(1).cpu())\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_model(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # 计算RMSE（将标签转回1-10）\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型一 ---\n",
    "print(\"开始训练模型一 (GloVe + BiLSTM)...\")\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_model(model1, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_model(model1, val_loader, criterion)\n",
    "    \n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 训练准确率: {train_acc*100:.2f}% | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. 在测试集上对模型一进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型一 测试集结果 -> 准确率: 19.62% | Macro-F1: 0.033 | RMSE: 2.495\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1, test_rmse = evaluate_model(model1, test_loader, criterion)\n",
    "print(f'模型一 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results = {}\n",
    "results['模型一 (GloVe + BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型二：BERT嵌入 + BiLSTM\n",
    "\n",
    "在这个模型中，我们使用预训练的BERT模型作为特征提取器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: transformers in /root/miniconda3/lib/python3.12/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从本地路径加载BERT分词器: /root/models/bert-base-uncased\n",
      "用于BERT的数据准备完成。\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- 参数配置 ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN_BERT = 256  # 算力原因调整到256\n",
    "BATCH_SIZE_BERT = 64  \n",
    "\n",
    "# --- BERT 分词器 ---\n",
    "local_bert_path = \"/root/models/bert-base-uncased\"\n",
    "\n",
    "if os.path.exists(local_bert_path):\n",
    "    print(f\"从本地路径加载BERT分词器: {local_bert_path}\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(local_bert_path)\n",
    "else:\n",
    "    print(f\"从网络下载BERT分词器: {BERT_MODEL_NAME}\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# --- 用于BERT的PyTorch数据集 ---\n",
    "class IMDBDatasetBERT(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 创建数据加载器 ---\n",
    "train_dataset_bert = IMDBDatasetBERT(df_train, tokenizer_bert, MAX_LEN_BERT)\n",
    "val_dataset_bert = IMDBDatasetBERT(df_val, tokenizer_bert, MAX_LEN_BERT)\n",
    "test_dataset_bert = IMDBDatasetBERT(df_test, tokenizer_bert, MAX_LEN_BERT)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=BATCH_SIZE_BERT, shuffle=True)\n",
    "val_loader_bert = DataLoader(val_dataset_bert, batch_size=BATCH_SIZE_BERT)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=BATCH_SIZE_BERT)\n",
    "print(\"用于BERT的数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. BERT+BiLSTM 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型二已创建\n"
     ]
    }
   ],
   "source": [
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                              hidden_dim,\n",
    "                              num_layers=n_layers,\n",
    "                              bidirectional=True,\n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 冻结BERT，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "if os.path.exists(local_bert_path):\n",
    "    bert_model = BertModel.from_pretrained(local_bert_path)\n",
    "else:\n",
    "    bert_model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# 冻结BERT的参数\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 实例化模型\n",
    "model2 = BertBiLSTMClassifier(bert_model, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "print(\"模型二已创建\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 训练与评估（模型二）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练模型二 (BERT + BiLSTM)...\n",
      "轮次: 01 | 训练损失: 1.836 | 验证损失: 1.679 | 验证准确率: 32.35% | 验证F1: 0.253 | 验证RMSE: 1.643\n",
      "轮次: 02 | 训练损失: 1.697 | 验证损失: 1.647 | 验证准确率: 33.12% | 验证F1: 0.235 | 验证RMSE: 1.595\n",
      "轮次: 03 | 训练损失: 1.652 | 验证损失: 1.640 | 验证准确率: 34.04% | 验证F1: 0.244 | 验证RMSE: 1.674\n"
     ]
    }
   ],
   "source": [
    "def train_bert_bilstm(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_bilstm(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型二 ---\n",
    "print(\"\\n开始训练模型二 (BERT + BiLSTM)...\")\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 减少训练轮数\n",
    "for epoch in range(3):\n",
    "    train_loss = train_bert_bilstm(model2, train_loader_bert, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_bert_bilstm(model2, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 在测试集上对模型二进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型二 测试集结果 -> 准确率: 33.70% | Macro-F1: 0.240 | RMSE: 1.729\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1, test_rmse = evaluate_bert_bilstm(model2, test_loader_bert, criterion)\n",
    "print(f'模型二 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型二 (BERT嵌入 + BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型三：微调BERT\n",
    "\n",
    "这是最常用且最强大的方法。我们采用一个带分类头的预训练BERT模型，并在我们的特定任务上对整个模型进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT微调模型已加载。\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# --- 加载模型 ---\n",
    "if os.path.exists(local_bert_path):\n",
    "    model3 = BertForSequenceClassification.from_pretrained(\n",
    "        local_bert_path,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ).to(DEVICE)\n",
    "else:\n",
    "    model3 = BertForSequenceClassification.from_pretrained(\n",
    "        BERT_MODEL_NAME,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "# --- 优化器与学习率调度器 ---\n",
    "optimizer = AdamW(model3.parameters(), lr=2e-5, eps=1e-8)\n",
    "EPOCHS_BERT_FINETUNE = 3\n",
    "total_steps = len(train_loader_bert) * EPOCHS_BERT_FINETUNE\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "print(\"BERT微调模型已加载。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 训练与评估（模型三）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练模型三 (微调BERT)...\n",
      "轮次: 01 | 训练损失: 1.675 | 验证损失: 1.528 | 验证准确率: 39.08% | 验证F1: 0.312 | 验证RMSE: 1.452\n",
      "轮次: 02 | 训练损失: 1.442 | 验证损失: 1.494 | 验证准确率: 41.01% | 验证F1: 0.366 | 验证RMSE: 1.415\n",
      "轮次: 03 | 训练损失: 1.323 | 验证损失: 1.509 | 验证准确率: 40.68% | 验证F1: 0.364 | 验证RMSE: 1.417\n"
     ]
    }
   ],
   "source": [
    "def train_bert_finetune(model, iterator, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_finetune(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型三 ---\n",
    "print(\"\\n开始训练模型三 (微调BERT)...\")\n",
    "for epoch in range(EPOCHS_BERT_FINETUNE):\n",
    "    train_loss = train_bert_finetune(model3, train_loader_bert, optimizer, scheduler, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_bert_finetune(model3, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 在测试集上对模型三进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型三 测试集结果 -> 准确率: 41.17% | Macro-F1: 0.361 | RMSE: 1.453\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1, test_rmse = evaluate_bert_finetune(model3, test_loader_bert, criterion)\n",
    "print(f'模型三 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型三 (微调BERT)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结与性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- IMDB-10测试集最终性能对比 ---\n",
      "                      Accuracy Macro-F1    RMSE\n",
      "模型一 (GloVe + BiLSTM)    19.62%   0.0328  2.4954\n",
      "模型二 (BERT嵌入 + BiLSTM)   33.70%   0.2401  1.7289\n",
      "模型三 (微调BERT)            41.17%   0.3610  1.4529\n",
      "\n",
      "结果已保存到 imdb_results.csv\n"
     ]
    }
   ],
   "source": [
    "# 创建结果DataFrame\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results['Accuracy'] = df_results['Accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "df_results['Macro-F1'] = df_results['Macro-F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_results['RMSE'] = df_results['RMSE'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"--- IMDB-10测试集最终性能对比 ---\")\n",
    "print(df_results)\n",
    "\n",
    "# 保存结果\n",
    "df_results.to_csv('imdb_results.csv')\n",
    "print(\"\\n结果已保存到 imdb_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
