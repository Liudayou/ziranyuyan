{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一：IMDB-10 情感分类 (优化版)\n",
    "\n",
    "本Notebook旨在完成IMDB电影评论的十分类情感（评分1-10）任务。我们将根据实验要求，实现并对比三种不同的深度学习模型。\n",
    "\n",
    "**优化说明:**\n",
    "此版本针对模型二和模型三的训练速度进行了优化，主要改动包括：\n",
    "1.  **数据预分词 (Pre-tokenization):** 将文本一次性转换为BERT输入格式，避免在训练循环中重复分词，消除CPU瓶颈。\n",
    "2.  **增大批次大小 (Batch Size):** 根据8GB显存配置，将BERT模型的批次大小从32提升至64，提高GPU利用率。\n",
    "3.  **添加`tqdm`进度条:** 为训练和评估过程添加了简洁的进度条，方便跟踪进度。\n",
    "4.  **启用CuDNN基准测试:** 优化底层计算效率。\n",
    "\n",
    "**实验方案：**\n",
    "1.  **模型一：** GloVe词向量 + BiLSTM + 全连接层\n",
    "2.  **模型二：** BERT-base 嵌入 + BiLSTM + 分类头\n",
    "3.  **模型三：** 微调 BERT-base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境设置与依赖安装\n",
    "\n",
    "首先，安装所有必需的Python库。请确保`tqdm`已安装 (`pip install tqdm`)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: (67426, 2)\n",
      "验证集大小: (8381, 2)\n",
      "测试集大小: (9112, 2)\n",
      "\n",
      "数据样本示例:\n",
      "                                                text  label\n",
      "0  i excepted a lot from this movie , and it did ...      9\n",
      "1  this movie is not worth seeing .   has no meri...      0\n",
      "2  this is a truly remarkable horror movie .   al...      9\n",
      "3  * minor spoilers * this movie is inept .   so ...      2\n",
      "4  this is a brilliant horror movie .   fans of t...      9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_imdb_data(file_path):\n",
    "    \"\"\"加载并解析IMDB数据文件，使用正则表达式查找标签。\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                match = re.search(r'\\t\\t(\\d+)\\t\\t', line)\n",
    "                if match:\n",
    "                    rating = int(match.group(1))\n",
    "                    text_start_index = match.end()\n",
    "                    text = line[text_start_index:].strip()\n",
    "                    text = text.replace('<sssss>', ' ').strip()\n",
    "                    labels.append(rating - 1)\n",
    "                    texts.append(text)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {file_path}。请确保数据文件在当前目录下。\")\n",
    "        return pd.DataFrame({'text': [], 'label': []})\n",
    "    \n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "    # 重置索引以确保后续 .loc[idx] 的正确性\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# 定义文件路径\n",
    "train_file = 'imdb.train.txt.ss'\n",
    "dev_file = 'imdb.dev.txt.ss'\n",
    "test_file = 'imdb.test.txt.ss'\n",
    "\n",
    "# 加载所有数据集\n",
    "df_train = load_imdb_data(train_file)\n",
    "df_val = load_imdb_data(dev_file)\n",
    "df_test = load_imdb_data(test_file)\n",
    "\n",
    "if not df_train.empty:\n",
    "    print(f\"训练集大小: {df_train.shape}\")\n",
    "    print(f\"验证集大小: {df_val.shape}\")\n",
    "    print(f\"测试集大小: {df_test.shape}\")\n",
    "    print(\"\\n数据样本示例:\")\n",
    "    print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型一：GloVe + BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe词向量与数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在构建词汇表...\n",
      "正在加载GloVe词向量...\n",
      "GloVe加载完成。\n",
      "\n",
      "数据准备完成。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 参数配置 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True # OPTIMIZATION\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE_LSTM = 64\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "MAX_LEN_LSTM = 512\n",
    "GLOVE_PATH = r\"D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\"\n",
    "\n",
    "# --- 文本分词器 ---\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9' ]+\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "# --- 构建词汇表 ---\n",
    "print(\"正在构建词汇表...\")\n",
    "word_counts = Counter()\n",
    "for text in df_train['text']:\n",
    "    word_counts.update(tokenizer(text))\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_idx = {word: i + 2 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<unk>'] = 1\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "\n",
    "# --- 加载GloVe词向量 ---\n",
    "print(\"正在加载GloVe词向量...\")\n",
    "glove_embeddings = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "try:\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            if len(parts) != EMBEDDING_DIM + 1:\n",
    "                continue\n",
    "            if word in word_to_idx:\n",
    "                try:\n",
    "                    vector = np.array(parts[1:], dtype=np.float32)\n",
    "                    glove_embeddings[word_to_idx[word]] = vector\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    glove_embeddings = torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "    print(\"GloVe加载完成。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"错误: GloVe文件未找到 {GLOVE_PATH}。模型一将无法运行。\")\n",
    "    glove_embeddings = torch.randn(VOCAB_SIZE, EMBEDDING_DIM) # 使用随机向量代替\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_to_idx, max_len):\n",
    "        self.df = dataframe\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        tokens = [self.word_to_idx.get(word, 1) for word in tokenizer(text)]\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        else:\n",
    "            tokens.extend([0] * (self.max_len - len(tokens)))\n",
    "        return torch.tensor(tokens), torch.tensor(label)\n",
    "\n",
    "# --- 创建DataLoaders ---\n",
    "train_dataset = IMDBDataset(df_train, word_to_idx, MAX_LEN_LSTM)\n",
    "val_dataset = IMDBDataset(df_val, word_to_idx, MAX_LEN_LSTM)\n",
    "test_dataset = IMDBDataset(df_test, word_to_idx, MAX_LEN_LSTM)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE_LSTM, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE_LSTM)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_LSTM)\n",
    "\n",
    "print(\"\\n数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 模型一结构定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                              bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "model1 = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT, glove_embeddings).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 训练与评估（模型一）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型一...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 01 | 训练损失: 2.075 | 验证损失: 1.989 | 验证Acc: 23.47% | 验证F1: 0.095 | 验证RMSE: 2.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 02 | 训练损失: 1.875 | 验证损失: 1.748 | 验证Acc: 30.04% | 验证F1: 0.210 | 验证RMSE: 1.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 03 | 训练损失: 1.743 | 验证损失: 1.654 | 验证Acc: 34.07% | 验证F1: 0.237 | 验证RMSE: 1.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 04 | 训练损失: 1.651 | 验证损失: 1.549 | 验证Acc: 38.15% | 验证F1: 0.305 | 验证RMSE: 1.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 05 | 训练损失: 1.576 | 验证损失: 1.535 | 验证Acc: 39.30% | 验证F1: 0.302 | 验证RMSE: 1.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型一 测试集结果 -> Acc: 39.07% | F1: 0.296 | RMSE: 1.579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # OPTIMIZATION: Add tqdm progress bar\n",
    "    for batch in tqdm(iterator, desc=\"Training (M1)\", leave=False):\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        # OPTIMIZATION: Add tqdm progress bar\n",
    "        for batch in tqdm(iterator, desc=\"Evaluating (M1)\", leave=False):\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 开始训练 ---\n",
    "print(\"开始训练模型一...\")\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "results = {}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train(model1, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate(model1, val_loader, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证Acc: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')\n",
    "\n",
    "test_loss, test_acc, test_f1, test_rmse = evaluate(model1, test_loader, criterion)\n",
    "print(f'\\n模型一 测试集结果 -> Acc: {test_acc*100:.2f}% | F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型一 (GloVe+BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型二 & 三：基于BERT的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. BERT数据准备 (已优化)\n",
    "\n",
    "**核心优化**：我们不再在`Dataset`中对每个样本进行实时分词。取而代之，我们先一次性地将整个文本数据集分词，并将结果（`input_ids`, `attention_mask`）存储起来。新的`Dataset`类将直接、快速地从这些预处理好的张量中读取数据，从而消除CPU瓶颈。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从本地路径加载BERT分词器: D:/models/bert-base-uncased\n",
      "正在对 67426 条数据进行预分词...\n",
      "正在对 8381 条数据进行预分词...\n",
      "正在对 9112 条数据进行预分词...\n",
      "\n",
      "用于BERT的数据准备完成 (已优化)。\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# --- 参数配置 ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN_BERT = 256 # OPTIMIZATION: 从512缩减到256，减少计算和显存，8GB显存下更安全\n",
    "BATCH_SIZE_BERT = 64 # OPTIMIZATION: 从32增加到64，提高GPU利用率\n",
    "\n",
    "local_bert_path = 'D:/models/bert-base-uncased'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(local_bert_path):\n",
    "    # 如果本地没有，尝试从Hugging Face Hub下载\n",
    "    print(f\"本地路径 {local_bert_path} 不存在，尝试从网络下载 {BERT_MODEL_NAME}...\")\n",
    "    try:\n",
    "        tokenizer_bert = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "        # 如果需要保存到本地\n",
    "        # tokenizer_bert.save_pretrained(local_bert_path)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"无法下载BERT模型，请检查网络或手动下载。错误: {e}\")\n",
    "else:\n",
    "    print(f\"从本地路径加载BERT分词器: {local_bert_path}\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(local_bert_path)\n",
    "\n",
    "# --- OPTIMIZATION: 预分词函数 ---\n",
    "def pre_tokenize_data(df, tokenizer, max_len):\n",
    "    print(f\"正在对 {len(df)} 条数据进行预分词...\")\n",
    "    # 使用tokenizer的批量编码功能，速度非常快\n",
    "    encodings = tokenizer.batch_encode_plus(\n",
    "        df['text'].tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask'], torch.tensor(df['label'].tolist())\n",
    "\n",
    "# --- OPTIMIZATION: 预分词所有数据集 ---\n",
    "train_input_ids, train_attention_mask, train_labels = pre_tokenize_data(df_train, tokenizer_bert, MAX_LEN_BERT)\n",
    "val_input_ids, val_attention_mask, val_labels = pre_tokenize_data(df_val, tokenizer_bert, MAX_LEN_BERT)\n",
    "test_input_ids, test_attention_mask, test_labels = pre_tokenize_data(df_test, tokenizer_bert, MAX_LEN_BERT)\n",
    "\n",
    "# --- OPTIMIZATION: 使用预分词数据的简化版Dataset ---\n",
    "class PretokenizedIMDBDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# --- 创建DataLoaders ---\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dataloader_args = {\n",
    "    \"batch_size\": BATCH_SIZE_BERT,\n",
    "    # 在Windows的Jupyter中，num_workers>0可能出问题，设为0最安全。预分词后，这里不再是瓶颈\n",
    "    \"num_workers\": 0, \n",
    "    \"pin_memory\": True if use_cuda else False\n",
    "}\n",
    "\n",
    "train_dataset_bert = PretokenizedIMDBDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset_bert = PretokenizedIMDBDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "test_dataset_bert = PretokenizedIMDBDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, shuffle=True, **dataloader_args)\n",
    "val_loader_bert = DataLoader(val_dataset_bert, **dataloader_args)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, **dataloader_args)\n",
    "\n",
    "print(\"\\n用于BERT的数据准备完成 。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 模型二：BERT嵌入 + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, \n",
    "                              bidirectional=True, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 冻结BERT，不计算梯度，节省计算资源\n",
    "        with torch.no_grad():\n",
    "            # BERT返回 last_hidden_state, pooler_output\n",
    "            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        # LSTM和FC层需要计算梯度\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# --- 实例化模型二 ---\n",
    "bert_model_for_bilstm = BertModel.from_pretrained(local_bert_path if os.path.exists(local_bert_path) else BERT_MODEL_NAME)\n",
    "# 冻结BERT参数\n",
    "for param in bert_model_for_bilstm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model2 = BertBiLSTMClassifier(bert_model_for_bilstm, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 训练与评估（模型二）(已优化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练模型二...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (M2):   0%|          | 0/1054 [00:00<?, ?it/s]e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 01 | 训练损失: 1.840 | 验证损失: 1.726 | 验证Acc: 30.29% | 验证F1: 0.210 | 验证RMSE: 1.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 02 | 训练损失: 1.702 | 验证损失: 1.645 | 验证Acc: 33.99% | 验证F1: 0.257 | 验证RMSE: 1.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 03 | 训练损失: 1.653 | 验证损失: 1.633 | 验证Acc: 32.88% | 验证F1: 0.259 | 验证RMSE: 1.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型二 测试集结果 -> Acc: 32.95% | F1: 0.252 | RMSE: 1.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_bert_based(model, iterator, optimizer, criterion, scaler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # OPTIMIZATION: Add tqdm progress bar\n",
    "    for batch in tqdm(iterator, desc=\"Training (M2)\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast(): # 混合精度\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_based(model, iterator, criterion, model_name=\"M2\"):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        # OPTIMIZATION: Add tqdm progress bar\n",
    "        for batch in tqdm(iterator, desc=f\"Evaluating ({model_name})\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            with autocast(): # 混合精度\n",
    "                predictions = model(input_ids, attention_mask)\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型二---\n",
    "print(\"\\n开始训练模型二...\")\n",
    "EPOCHS_BERT = 3\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS_BERT):\n",
    "    train_loss = train_bert_based(model2, train_loader_bert, optimizer, criterion, scaler)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_bert_based(model2, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证Acc: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')\n",
    "\n",
    "test_loss, test_acc, test_f1, test_rmse = evaluate_bert_based(model2, test_loader_bert, criterion)\n",
    "print(f'\\n模型二 测试集结果 -> Acc: {test_acc*100:.2f}% | F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型二 (BERT嵌入+BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. 模型三：微调BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at D:/models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# --- 实例化模型三 ---\n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    local_bert_path if os.path.exists(local_bert_path) else BERT_MODEL_NAME,\n",
    "    num_labels=NUM_CLASSES,\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- 优化器与学习率调度器 ---\n",
    "optimizer3 = AdamW(model3.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_loader_bert) * EPOCHS_BERT\n",
    "scheduler3 = get_linear_schedule_with_warmup(optimizer3, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "criterion3 = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. 训练与评估（模型三）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练模型三...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 01 | 训练损失: 1.691 | 验证损失: 1.531 | 验证Acc: 38.92% | 验证F1: 0.318 | 验证RMSE: 1.457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 02 | 训练损失: 1.456 | 验证损失: 1.495 | 验证Acc: 40.22% | 验证F1: 0.333 | 验证RMSE: 1.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 03 | 训练损失: 1.333 | 验证损失: 1.505 | 验证Acc: 40.87% | 验证F1: 0.365 | 验证RMSE: 1.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "模型三 测试集结果 -> Acc: 40.50% | F1: 0.353 | RMSE: 1.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def train_finetune(model, iterator, optimizer, scheduler, criterion, scaler):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # OPTIMIZATION: Add tqdm progress bar\n",
    "    for batch in tqdm(iterator, desc=\"Training (M3)\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        with autocast(): # 混合精度\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_finetune(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        # OPTIMIZATION: Add tqdm progress bar\n",
    "        for batch in tqdm(iterator, desc=\"Evaluating (M3)\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            with autocast(): # 混合精度\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型三 ---\n",
    "print(\"\\n开始训练模型三...\")\n",
    "scaler_finetune = GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS_BERT):\n",
    "    train_loss = train_finetune(model3, train_loader_bert, optimizer3, scheduler3, criterion3, scaler_finetune)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_finetune(model3, val_loader_bert, criterion3)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证Acc: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')\n",
    "\n",
    "test_loss, test_acc, test_f1, test_rmse = evaluate_finetune(model3, test_loader_bert, criterion3)\n",
    "print(f'\\n模型三 测试集结果 -> Acc: {test_acc*100:.2f}% | F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型三 (微调BERT)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 结果汇总与分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- IMDB-10测试集最终性能对比 ---\n",
      "                    Accuracy Macro-F1    RMSE\n",
      "模型一 (GloVe+BiLSTM)    39.07%   0.2960  1.5786\n",
      "模型二 (BERT嵌入+BiLSTM)   32.95%   0.2524  1.7627\n",
      "模型三 (微调BERT)          40.50%   0.3531  1.4459\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results).T\n",
    "df_results['Accuracy'] = df_results['Accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "df_results['Macro-F1'] = df_results['Macro-F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_results['RMSE'] = df_results['RMSE'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"--- IMDB-10测试集最终性能对比 ---\")\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
