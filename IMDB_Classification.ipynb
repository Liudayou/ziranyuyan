{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB-10 情感分类 —— 任务一实验报告\n",
    "\n",
    "#### 数据集说明\n",
    "本实验使用用户提供的三个文件作为数据集：\n",
    "- imdb.train.txt.ss\n",
    "- imdb.dev.txt.ss\n",
    "- imdb.test.txt.ss\n",
    "\n",
    "每个文件包含若干样本。请确保文件格式为：每行一条，通常格式为 `<label>\\t<text>` 或 `<text>\\t<label>`。如有不同请相应调整代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from torch.optim import AdamW  # 从 PyTorch 导入 AdamW\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# 设置随机种子确保可复现\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 D:\\期末大作业\\期末大作业\\数据集\\imdb\\imdb.train.txt.ss 的前 5 行:\n",
      "行 1: 'ur2480402/\\t\\t\\\\tt0119485\\t\\t10\\t\\ti excepted a lot from this movie , and it did deliver . <sssss> there is some great buddhist wisdom in this movie . <sssss> the real dalai lama is a very interesting person , and i think there is a lot of wisdom in buddhism . <sssss> the music , of course , sounds like because it is by philip glass . <sssss> this adds to the beauty of the movie . <sssss> whereas other biographies of famous people tend to get very poor this movie always stays focused and gives a good and honest portrayal of the dalai lama . <sssss> all things being equal , it is a great movie , and i really enjoyed it . <sssss> it is not like taxi driver of course but as a biography of a famous person it is really a great film indeed . \\n'\n",
      "行 1 (常规显示): ur2480402/\t\t\\tt0119485\t\t10\t\ti excepted a lot from this movie , and it did deliver . <sssss> there is some great buddhist wisdom in this movie . <sssss> the real dalai lama is a very interesting person , and i think there is a lot of wisdom in buddhism . <sssss> the music , of course , sounds like because it is by philip glass . <sssss> this adds to the beauty of the movie . <sssss> whereas other biographies of famous people tend to get very poor this movie always stays focused and gives a good and honest portrayal of the dalai lama . <sssss> all things being equal , it is a great movie , and i really enjoyed it . <sssss> it is not like taxi driver of course but as a biography of a famous person it is really a great film indeed . \n",
      "\n",
      "行 2: 'ur2480402/\\t\\t\\\\tt0109836\\t\\t1\\t\\tthis movie is not worth seeing . <sssss> has no merits what so ever . <sssss> does not make sense . <sssss> a total waste of time . <sssss> i don ´ t know why brannagh bothered to try and make this . <sssss> de niro is a joke as the monster . <sssss> read a horror story instead of this exploitation of viewers . <sssss> simply not worth our time . \\n'\n",
      "行 2 (常规显示): ur2480402/\t\t\\tt0109836\t\t1\t\tthis movie is not worth seeing . <sssss> has no merits what so ever . <sssss> does not make sense . <sssss> a total waste of time . <sssss> i don ´ t know why brannagh bothered to try and make this . <sssss> de niro is a joke as the monster . <sssss> read a horror story instead of this exploitation of viewers . <sssss> simply not worth our time . \n",
      "\n",
      "行 3: \"ur2480402/\\t\\t\\\\tt0093177\\t\\t10\\t\\tthis is a truly remarkable horror movie . <sssss> although difficult to watch , especially twice , when you ´ ve realized how mean and groundbreaking it is . <sssss> the story is like ingmar bergman combined with the most sublime horror . <sssss> very surprising . <sssss> a truly astonishing vision . <sssss> clive barker is a most remarkable and visionary person . <sssss> although a movie like `` the ring '' -lrb- japanese version -rrb- is more convincing , this movie really astonishes on its own term . <sssss> it is the story behind which is so truly amazing , and you bring this out of the experience - that is , you remember the vision long after having seen this movie . <sssss> the sequal is not bad either , although the story is a bit worse . <sssss> many further sequels are completely useless though . \\n\"\n",
      "行 3 (常规显示): ur2480402/\t\t\\tt0093177\t\t10\t\tthis is a truly remarkable horror movie . <sssss> although difficult to watch , especially twice , when you ´ ve realized how mean and groundbreaking it is . <sssss> the story is like ingmar bergman combined with the most sublime horror . <sssss> very surprising . <sssss> a truly astonishing vision . <sssss> clive barker is a most remarkable and visionary person . <sssss> although a movie like `` the ring '' -lrb- japanese version -rrb- is more convincing , this movie really astonishes on its own term . <sssss> it is the story behind which is so truly amazing , and you bring this out of the experience - that is , you remember the vision long after having seen this movie . <sssss> the sequal is not bad either , although the story is a bit worse . <sssss> many further sequels are completely useless though . \n",
      "\n",
      "行 4: \"ur2480402/\\t\\t\\\\tt0285346\\t\\t3\\t\\t* minor spoilers * this movie is inept . <sssss> so inept that one should have empathy , something which bundy could not have . <sssss> the acting by main actor is actually quite good and there are special effects by the legend tom `` friday the 13th '' savini . <sssss> that 's about all the good things there is to say about this movie . <sssss> one will learn less than zero about bundy . <sssss> some of the clips from jail where probably the best . <sssss> e.g. where he says : `` i know i 'm not like other people . <sssss> how i can not feel sympathy with others . <sssss> but i 'm still human . '' <sssss> this seems to be the essence of bundy 's psychology . <sssss> the death penalty , in itself a monstrous subject , is treated worse than inept in this movie . <sssss> for example , a beautiful woman is apparently executing the . <sssss> yes , so ? <sssss> what has that got to do with bundy 's misogyny or anything ? <sssss> the subject is so monumental and a kubrick or coppola could have gotten an astonishing masterpiece out of this material . <sssss> this movie should be avoided at all costs . <sssss> especially by those interested in the truth about ted bundy . <sssss> by the way , i 've just noticed because i saw the lead actor in `` creature '' that he does n't look anything like bundy . <sssss> they might have picked one that was closer to his physical appearance also . \\n\"\n",
      "行 4 (常规显示): ur2480402/\t\t\\tt0285346\t\t3\t\t* minor spoilers * this movie is inept . <sssss> so inept that one should have empathy , something which bundy could not have . <sssss> the acting by main actor is actually quite good and there are special effects by the legend tom `` friday the 13th '' savini . <sssss> that 's about all the good things there is to say about this movie . <sssss> one will learn less than zero about bundy . <sssss> some of the clips from jail where probably the best . <sssss> e.g. where he says : `` i know i 'm not like other people . <sssss> how i can not feel sympathy with others . <sssss> but i 'm still human . '' <sssss> this seems to be the essence of bundy 's psychology . <sssss> the death penalty , in itself a monstrous subject , is treated worse than inept in this movie . <sssss> for example , a beautiful woman is apparently executing the . <sssss> yes , so ? <sssss> what has that got to do with bundy 's misogyny or anything ? <sssss> the subject is so monumental and a kubrick or coppola could have gotten an astonishing masterpiece out of this material . <sssss> this movie should be avoided at all costs . <sssss> especially by those interested in the truth about ted bundy . <sssss> by the way , i 've just noticed because i saw the lead actor in `` creature '' that he does n't look anything like bundy . <sssss> they might have picked one that was closer to his physical appearance also . \n",
      "\n",
      "行 5: 'ur2480402/\\t\\t\\\\tt0185937\\t\\t10\\t\\tthis is a brilliant horror movie . <sssss> fans of the genre knows this . <sssss> the reason there has been so much criticism of the movie is that non-horror fans where lured to see it as a semi-documentary ! <sssss> this is pretty naive or course , i mean to think that it is reality ... !!!!!!!!!!!!!!!!! <sssss> ... ... -lsb- this note was found in the woods last year -rsb- \\n'\n",
      "行 5 (常规显示): ur2480402/\t\t\\tt0185937\t\t10\t\tthis is a brilliant horror movie . <sssss> fans of the genre knows this . <sssss> the reason there has been so much criticism of the movie is that non-horror fans where lured to see it as a semi-documentary ! <sssss> this is pretty naive or course , i mean to think that it is reality ... !!!!!!!!!!!!!!!!! <sssss> ... ... -lsb- this note was found in the woods last year -rsb- \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def peek_file(filepath, n=5):\n",
    "    \"\"\"打印文件的前n行\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            print(f\"文件 {filepath} 的前 {n} 行:\")\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= n:\n",
    "                    break\n",
    "                # 使用repr()显示所有字符，包括制表符和换行符\n",
    "                print(f\"行 {i+1}: {repr(line)}\")\n",
    "                # 也打印常规格式，便于阅读\n",
    "                print(f\"行 {i+1} (常规显示): {line}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"读取文件时出错: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 使用绝对路径\n",
    "train_path = r\"D:\\期末大作业\\期末大作业\\数据集\\imdb\\imdb.train.txt.ss\"\n",
    "peek_file(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 读取你的txt数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析结果: 评分=10, 标签=9, 评论=i excepted a lot from this movie , and it did deli...\n",
      "解析结果: 评分=1, 标签=1, 评论=this movie is not worth seeing . <sssss> has no me...\n",
      "解析结果: 评分=10, 标签=9, 评论=this is a truly remarkable horror movie . <sssss> ...\n",
      "成功加载 67426 条数据\n",
      "解析结果: 评分=7, 标签=7, 评论=i was born in 1976 , so i sort of grew up with wil...\n",
      "解析结果: 评分=4, 标签=4, 评论=this movie has good performances by sylvester stal...\n",
      "解析结果: 评分=4, 标签=4, 评论=this is a typical horror movie of that period . <s...\n",
      "成功加载 8381 条数据\n",
      "解析结果: 评分=10, 标签=9, 评论=this is a stunningly beautiful movie . <sssss> the...\n",
      "解析结果: 评分=10, 标签=9, 评论=this is quite possible one of the best movies made...\n",
      "解析结果: 评分=10, 标签=9, 评论=i was astonished to see the relatively low rating ...\n",
      "成功加载 9112 条数据\n",
      "train: ('i excepted a lot from this movie , and it did deliver . <sssss> there is some great buddhist wisdom in this movie . <sssss> the real dalai lama is a very interesting person , and i think there is a lot of wisdom in buddhism . <sssss> the music , of course , sounds like because it is by philip glass . <sssss> this adds to the beauty of the movie . <sssss> whereas other biographies of famous people tend to get very poor this movie always stays focused and gives a good and honest portrayal of the dalai lama . <sssss> all things being equal , it is a great movie , and i really enjoyed it . <sssss> it is not like taxi driver of course but as a biography of a famous person it is really a great film indeed .', 9)\n",
      "成功加载 67426 条数据\n",
      "解析结果: 评分=7, 标签=7, 评论=i was born in 1976 , so i sort of grew up with wil...\n",
      "解析结果: 评分=4, 标签=4, 评论=this movie has good performances by sylvester stal...\n",
      "解析结果: 评分=4, 标签=4, 评论=this is a typical horror movie of that period . <s...\n",
      "成功加载 8381 条数据\n",
      "解析结果: 评分=10, 标签=9, 评论=this is a stunningly beautiful movie . <sssss> the...\n",
      "解析结果: 评分=10, 标签=9, 评论=this is quite possible one of the best movies made...\n",
      "解析结果: 评分=10, 标签=9, 评论=i was astonished to see the relatively low rating ...\n",
      "成功加载 9112 条数据\n",
      "train: ('i excepted a lot from this movie , and it did deliver . <sssss> there is some great buddhist wisdom in this movie . <sssss> the real dalai lama is a very interesting person , and i think there is a lot of wisdom in buddhism . <sssss> the music , of course , sounds like because it is by philip glass . <sssss> this adds to the beauty of the movie . <sssss> whereas other biographies of famous people tend to get very poor this movie always stays focused and gives a good and honest portrayal of the dalai lama . <sssss> all things being equal , it is a great movie , and i really enjoyed it . <sssss> it is not like taxi driver of course but as a biography of a famous person it is really a great film indeed .', 9)\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_file(path):\n",
    "    data = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                # 正则表达式查找评分模式（前后有制表符的数字）\n",
    "                import re\n",
    "                match = re.search(r'\\t\\t(\\d+)\\t\\t', line)\n",
    "                \n",
    "                if not match:\n",
    "                    print(f\"行 {line_num} 未找到评分模式: {line[:50]}...\")\n",
    "                    continue\n",
    "                \n",
    "                # 提取评分和位置\n",
    "                rating = match.group(1)\n",
    "                \n",
    "                # 评论文本在评分之后\n",
    "                review_start = match.end()\n",
    "                review = line[review_start:].strip()\n",
    "                \n",
    "                # 将评分转换为整数（映射到0-9区间）\n",
    "                rating_int = int(rating)\n",
    "                label = rating_int - 1 if rating_int == 10 else rating_int\n",
    "                \n",
    "                data.append((review, label))\n",
    "                \n",
    "                # 打印前几个解析结果（用于调试）\n",
    "                if line_num <= 3:\n",
    "                    print(f\"解析结果: 评分={rating}, 标签={label}, 评论={review[:50]}...\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"行 {line_num} 处理错误: {str(e)}\")\n",
    "    \n",
    "    print(f\"成功加载 {len(data)} 条数据\")\n",
    "    return data\n",
    "\n",
    "# 路径按实际文件所在路径调整\n",
    "train_path = r\"D:\\期末大作业\\期末大作业\\数据集\\imdb\\imdb.train.txt.ss\"\n",
    "dev_path = r\"D:\\期末大作业\\期末大作业\\数据集\\imdb\\imdb.dev.txt.ss\"\n",
    "test_path = r\"D:\\期末大作业\\期末大作业\\数据集\\imdb\\imdb.test.txt.ss\"\n",
    "\n",
    "train_data = load_imdb_file(train_path)\n",
    "dev_data = load_imdb_file(dev_path)\n",
    "test_data = load_imdb_file(test_path)\n",
    "\n",
    "print('train:', train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 分词、词表构建与padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "max_len = 256\n",
    "\n",
    "def tokenize_and_cut(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return tokens[:max_len]\n",
    "\n",
    "# 构建词表（用全部train+dev数据）\n",
    "all_tokens = set()\n",
    "for text, _ in train_data + dev_data:\n",
    "    all_tokens.update(tokenize_and_cut(text))\n",
    "vocab = {w: i+2 for i, w in enumerate(sorted(all_tokens))}\n",
    "vocab['<pad>'] = 0\n",
    "vocab['<unk>'] = 1\n",
    "\n",
    "def encode(text):\n",
    "    tokens = tokenize_and_cut(text)\n",
    "    ids = [vocab.get(t, 1) for t in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [0] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model 1: GloVe (300d) + BiLSTM + 全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查GloVe文件: D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\n",
      "GloVe文件已找到!\n",
      "正在加载GloVe词向量...\n",
      "成功加载GloVe词向量!\n",
      "成功加载GloVe词向量!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from torchtext.vocab import GloVe  # 确保正确导入GloVe\n",
    "\n",
    "# 你的GloVe文件的实际路径\n",
    "glove_file_path = r\"D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\"\n",
    "\n",
    "# 设置标准缓存目录\n",
    "cache_dir = os.path.expanduser(\"~/.vector_cache\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# 检查GloVe文件是否存在\n",
    "print(f\"检查GloVe文件: {glove_file_path}\")\n",
    "if os.path.exists(glove_file_path):\n",
    "    print(\"GloVe文件已找到!\")\n",
    "    \n",
    "    # 确保缓存目录中有正确命名的文件\n",
    "    cache_file = os.path.join(cache_dir, \"glove.840B.300d.txt\")\n",
    "    if not os.path.exists(cache_file):\n",
    "        print(f\"在缓存目录中创建GloVe文件的符号链接...\")\n",
    "        try:\n",
    "            # 在Windows上创建符号链接（需要管理员权限）\n",
    "            # 如果没有权限，可以使用复制，但会占用更多空间\n",
    "            if os.name == 'nt':  # Windows\n",
    "                # 尝试创建符号链接，如果失败则复制文件\n",
    "                try:\n",
    "                    os.symlink(glove_file_path, cache_file)\n",
    "                    print(\"成功创建符号链接\")\n",
    "                except OSError:\n",
    "                    print(\"无法创建符号链接，将复制文件（这可能需要一些时间）\")\n",
    "                    # 不复制文件，而是告诉用户设置环境变量\n",
    "                    print(\"正在设置环境变量，指向你的GloVe文件...\")\n",
    "            else:  # Unix/Linux\n",
    "                os.symlink(glove_file_path, cache_file)\n",
    "        except Exception as e:\n",
    "            print(f\"创建链接或复制文件时出错: {str(e)}\")\n",
    "else:\n",
    "    print(f\"警告: 找不到GloVe文件: {glove_file_path}\")\n",
    "    print(\"将尝试自动下载，但这可能会很慢\")\n",
    "\n",
    "# 设置环境变量，告诉torchtext在哪里找到GloVe文件\n",
    "os.environ['TORCH_HOME'] = os.path.dirname(os.path.dirname(glove_file_path))\n",
    "vectors_cache = os.path.dirname(glove_file_path)\n",
    "\n",
    "# 尝试加载GloVe词向量\n",
    "print(\"正在加载GloVe词向量...\")\n",
    "try:\n",
    "    # 使用你自己的向量目录\n",
    "    glove = GloVe(name='840B', dim=300, cache=vectors_cache)\n",
    "    print(\"成功加载GloVe词向量!\")\n",
    "except Exception as e:\n",
    "    print(f\"加载GloVe出错: {str(e)}\")\n",
    "    print(\"将使用随机初始化的词向量代替\")\n",
    "    # 创建一个假的glove对象，只包含必要的属性\n",
    "    class DummyGlove:\n",
    "        def __init__(self):\n",
    "            self.stoi = {}  # 空字典\n",
    "        def __getitem__(self, word):\n",
    "            return torch.zeros(300)  # 返回零向量\n",
    "    glove = DummyGlove()\n",
    "embedding_matrix = np.zeros((len(vocab), 300))\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove.stoi:\n",
    "        embedding_matrix[idx] = glove[word].numpy()\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(300, ))\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        return torch.tensor(encode(text)), torch.tensor(label)\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "dev_dataset = IMDBDataset(dev_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "是否使用GPU: True\n",
      "GPU名称: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "当前GPU内存使用情况: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        num_embeddings, emb_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(num_embeddings, emb_dim)\n",
    "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 检查并打印设备信息\n",
    "print(\"使用设备:\", device)\n",
    "print(\"是否使用GPU:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU名称:\", torch.cuda.get_device_name(0))\n",
    "    print(\"当前GPU内存使用情况:\", torch.cuda.memory_allocated(0)/1024**2, \"MB\")\n",
    "\n",
    "model1 = BiLSTMClassifier(embedding_matrix, num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    processed_batches = 0\n",
    "    print(f\"开始训练，共 {len(loader)} 批次...\")\n",
    "    \n",
    "    for i, (x, y) in enumerate(tqdm(loader)):\n",
    "        if i == 0:\n",
    "            print(f\"第一批次数据形状: x={x.shape}, y={y.shape}\")\n",
    "        \n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*x.size(0)\n",
    "        processed_batches += 1\n",
    "        \n",
    "        # 每100批次打印一次进度\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"处理了 {i+1}/{len(loader)} 批次...\")\n",
    "    \n",
    "    print(f\"训练完成，共处理 {processed_batches} 批次\")\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"开始评估，共 {len(loader)} 批次...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total += y.size(0)\n",
    "            \n",
    "            # 每50批次打印一次进度\n",
    "            if (i+1) % 50 == 0:\n",
    "                print(f\"评估了 {i+1}/{len(loader)} 批次...\")\n",
    "    \n",
    "    acc = correct/total\n",
    "    print(f\"评估完成，准确率: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 67426\n",
      "批次大小: 32\n",
      "总批次数: 2108\n",
      "模型参数数量: 24341490\n",
      "开始训练 Model 1...\n",
      "开始 Epoch 1/2...\n",
      "开始训练，共 2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2108 [00:00<04:06,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一批次数据形状: x=torch.Size([32, 256]), y=torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 114/2108 [00:01<00:23, 85.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 100/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 213/2108 [00:02<00:21, 87.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 200/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 312/2108 [00:03<00:20, 86.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 300/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 412/2108 [00:05<00:19, 86.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 400/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 511/2108 [00:06<00:18, 87.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 500/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 610/2108 [00:07<00:17, 86.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 600/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 712/2108 [00:08<00:15, 88.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 700/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 812/2108 [00:09<00:14, 87.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 800/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 911/2108 [00:10<00:13, 86.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 900/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1011/2108 [00:11<00:12, 86.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1000/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1110/2108 [00:13<00:11, 85.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1100/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 1209/2108 [00:14<00:10, 86.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1200/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1317/2108 [00:15<00:09, 87.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1300/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1416/2108 [00:16<00:07, 87.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1400/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1515/2108 [00:17<00:06, 87.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1500/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1615/2108 [00:18<00:05, 88.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1600/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 1714/2108 [00:19<00:04, 87.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1700/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1813/2108 [00:21<00:03, 88.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1800/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1913/2108 [00:22<00:02, 88.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1900/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 2013/2108 [00:23<00:01, 87.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 2000/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2108/2108 [00:24<00:00, 86.22it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 2100/2108 批次...\n",
      "训练完成，共处理 2108 批次\n",
      "训练完成，loss=1.4709\n",
      "开始验证...\n",
      "开始评估，共 66 批次...\n",
      "评估了 50/66 批次...\n",
      "评估了 50/66 批次...\n",
      "评估完成，准确率: 0.3830\n",
      "验证完成，acc=0.3830\n",
      "[Model1][Epoch 1/2] train_loss=1.4709, dev_acc=0.3830\n",
      "开始 Epoch 2/2...\n",
      "开始训练，共 2108 批次...\n",
      "评估完成，准确率: 0.3830\n",
      "验证完成，acc=0.3830\n",
      "[Model1][Epoch 1/2] train_loss=1.4709, dev_acc=0.3830\n",
      "开始 Epoch 2/2...\n",
      "开始训练，共 2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2108 [00:00<00:29, 71.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一批次数据形状: x=torch.Size([32, 256]), y=torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 115/2108 [00:01<00:23, 86.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 100/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 207/2108 [00:02<00:21, 87.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 200/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 315/2108 [00:03<00:20, 87.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 300/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 414/2108 [00:04<00:19, 86.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 400/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 513/2108 [00:05<00:18, 87.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 500/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 612/2108 [00:07<00:17, 86.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 600/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 713/2108 [00:08<00:15, 88.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 700/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 812/2108 [00:09<00:14, 86.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 800/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 911/2108 [00:10<00:13, 85.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 900/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1012/2108 [00:11<00:12, 87.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1000/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1111/2108 [00:12<00:11, 87.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1100/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1219/2108 [00:14<00:10, 87.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1200/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1313/2108 [00:15<00:08, 88.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1300/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1412/2108 [00:16<00:08, 86.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1400/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1511/2108 [00:17<00:06, 88.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1500/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 1610/2108 [00:18<00:05, 86.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1600/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1709/2108 [00:19<00:04, 85.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1700/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1817/2108 [00:20<00:03, 86.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1800/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1916/2108 [00:22<00:02, 88.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 1900/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2016/2108 [00:23<00:01, 87.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 2000/2108 批次...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2108/2108 [00:24<00:00, 86.89it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理了 2100/2108 批次...\n",
      "训练完成，共处理 2108 批次\n",
      "训练完成，loss=1.4022\n",
      "开始验证...\n",
      "开始评估，共 66 批次...\n",
      "评估了 50/66 批次...\n",
      "评估了 50/66 批次...\n",
      "评估完成，准确率: 0.3890\n",
      "验证完成，acc=0.3890\n",
      "[Model1][Epoch 2/2] train_loss=1.4022, dev_acc=0.3890\n",
      "评估完成，准确率: 0.3890\n",
      "验证完成，acc=0.3890\n",
      "[Model1][Epoch 2/2] train_loss=1.4022, dev_acc=0.3890\n"
     ]
    }
   ],
   "source": [
    "# 检查数据集和模型\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"批次大小: {train_loader.batch_size}\")\n",
    "print(f\"总批次数: {len(train_loader)}\")\n",
    "print(f\"模型参数数量: {sum(p.numel() for p in model1.parameters())}\")\n",
    "\n",
    "# 训练\n",
    "EPOCHS = 2\n",
    "print(\"开始训练 Model 1...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"开始 Epoch {epoch+1}/{EPOCHS}...\")\n",
    "    train_loss = train_epoch(model1, train_loader, criterion, optimizer)\n",
    "    print(f\"训练完成，loss={train_loss:.4f}\")\n",
    "    \n",
    "    print(f\"开始验证...\")\n",
    "    dev_acc = evaluate(model1, dev_loader)\n",
    "    print(f\"验证完成，acc={dev_acc:.4f}\")\n",
    "    \n",
    "    print(f\"[Model1][Epoch {epoch+1}/{EPOCHS}] train_loss={train_loss:.4f}, dev_acc={dev_acc:.4f}\")\n",
    "    \n",
    "    # 强制刷新输出\n",
    "    import sys\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model 2: BERT-base 嵌入 + BiLSTM + 分类头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设你已将模型文件下载到以下目录\n",
    "local_model_path = \"D:/models/bert-base-uncased\"  # 修改为你的实际路径\n",
    "\n",
    "# 检查BERT模型文件是否存在\n",
    "required_files = ['config.json', 'pytorch_model.bin', 'vocab.txt', 'tokenizer_config.json']\n",
    "missing_files = [f for f in required_files if not os.path.exists(os.path.join(local_model_path, f))]\n",
    "if missing_files:\n",
    "    print(f\"警告: BERT模型文件夹 {local_model_path} 中缺少以下文件:\")\n",
    "    for f in missing_files:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"这将触发自动下载，请从Hugging Face下载: https://huggingface.co/bert-base-uncased/tree/main\")\n",
    "\n",
    "# 从本地加载 tokenizer 和模型\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(local_model_path)\n",
    "bert_model = BertModel.from_pretrained(local_model_path).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "class BERTLSTMDataset(Dataset):\n",
    "    def __init__(self, data, max_len=128):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        encoding = bert_tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(label)\n",
    "\n",
    "def collate_bert(batch):\n",
    "    input_ids = torch.stack([b[0] for b in batch])\n",
    "    attention_mask = torch.stack([b[1] for b in batch])\n",
    "    labels = torch.stack([b[2] for b in batch])\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "# 数据集和数据加载器代码不需要修改\n",
    "train_bert_dataset = BERTLSTMDataset(train_data)\n",
    "dev_bert_dataset = BERTLSTMDataset(dev_data)\n",
    "test_bert_dataset = BERTLSTMDataset(test_data)\n",
    "\n",
    "train_bert_loader = DataLoader(train_bert_dataset, batch_size=16, shuffle=True, collate_fn=collate_bert)\n",
    "dev_bert_loader = DataLoader(dev_bert_dataset, batch_size=64, collate_fn=collate_bert)\n",
    "test_bert_loader = DataLoader(test_bert_dataset, batch_size=64, collate_fn=collate_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, num_classes)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            x = outputs.last_hidden_state\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "model2 = BERTBiLSTMClassifier(bert_model, num_classes=10).to(device)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "criterion2 = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_bert(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attn_mask, y in tqdm(loader):\n",
    "        input_ids, attn_mask, y = input_ids.to(device), attn_mask.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attn_mask)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*input_ids.size(0)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "def evaluate_bert(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attn_mask, y in loader:\n",
    "            input_ids, attn_mask, y = input_ids.to(device), attn_mask.to(device), y.to(device)\n",
    "            logits = model(input_ids, attn_mask)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4215 [00:00<?, ?it/s]e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 4215/4215 [08:44<00:00,  8.04it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model2][Epoch 0] train_loss=1.7394, dev_acc=0.3261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4215/4215 [08:41<00:00,  8.08it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model2][Epoch 1] train_loss=1.6429, dev_acc=0.3503\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch_bert(model2, train_bert_loader, criterion2, optimizer2)\n",
    "    dev_acc = evaluate_bert(model2, dev_bert_loader)\n",
    "    print(f\"[Model2][Epoch {epoch}] train_loss={train_loss:.4f}, dev_acc={dev_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 3: BERT-base 微调 + [CLS] + 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at D:/models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 使用本地模型路径替代'bert-base-uncased'\n",
    "local_model_path = \"D:/models/bert-base-uncased\"  # 修改为你的实际路径\n",
    "\n",
    "# 检查BERT模型文件是否存在（同上）\n",
    "if missing_files:\n",
    "    print(\"BERT分类模型将从网络下载\")\n",
    "\n",
    "model3 = BertForSequenceClassification.from_pretrained(local_model_path, num_labels=10).to(device)\n",
    "optimizer3 = AdamW(model3.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_model3(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attn_mask, y in tqdm(loader):\n",
    "        input_ids, attn_mask, y = input_ids.to(device), attn_mask.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=y)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()*input_ids.size(0)\n",
    "    return total_loss/len(loader.dataset)\n",
    "\n",
    "def evaluate_model3(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attn_mask, y in loader:\n",
    "            input_ids, attn_mask, y = input_ids.to(device), attn_mask.to(device), y.to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "            logits = outputs.logits\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred==y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4215/4215 [17:28<00:00,  4.02it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model3][Epoch 0] train_loss=1.6351, dev_acc=0.3854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4215/4215 [17:28<00:00,  4.02it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model3][Epoch 1] train_loss=1.4364, dev_acc=0.3896\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch_model3(model3, train_bert_loader, optimizer3)\n",
    "    dev_acc = evaluate_model3(model3, dev_bert_loader)\n",
    "    print(f\"[Model3][Epoch {epoch}] train_loss={train_loss:.4f}, dev_acc={dev_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 测试集评估与结果对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始评估，共 72 批次...\n",
      "评估了 50/72 批次...\n",
      "评估了 50/72 批次...\n",
      "评估完成，准确率: 0.3917\n",
      "评估完成，准确率: 0.3917\n",
      "Test accuracy: Model1=0.3917, Model2=0.3509, Model3=0.3808\n",
      "\n",
      "设备使用情况:\n",
      "使用设备: cuda\n",
      "GPU名称: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU内存使用: 2228.89 MB / 8187.50 MB\n",
      "Test accuracy: Model1=0.3917, Model2=0.3509, Model3=0.3808\n",
      "\n",
      "设备使用情况:\n",
      "使用设备: cuda\n",
      "GPU名称: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU内存使用: 2228.89 MB / 8187.50 MB\n"
     ]
    }
   ],
   "source": [
    "test_acc1 = evaluate(model1, test_loader)\n",
    "test_acc2 = evaluate_bert(model2, test_bert_loader)\n",
    "test_acc3 = evaluate_model3(model3, test_bert_loader)\n",
    "print(f\"Test accuracy: Model1={test_acc1:.4f}, Model2={test_acc2:.4f}, Model3={test_acc3:.4f}\")\n",
    "\n",
    "# 显示设备使用情况\n",
    "print(\"\\n设备使用情况:\")\n",
    "print(f\"使用设备: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU内存使用: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB / {torch.cuda.get_device_properties(0).total_memory/1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"使用CPU训练 - 模型训练速度会较慢\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
