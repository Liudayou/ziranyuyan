{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务二：SNLI蕴含识别\n",
    "\n",
    "本Notebook为自然语言推理（NLI）任务，在斯坦福自然语言推理（SNLI）数据集上，实现并对比三种不同的深度学习模型。\n",
    "\n",
    "三个模型分别是：\n",
    "1.  **模型一：** GloVe嵌入 + BiLSTM\n",
    "2.  **模型二：** BERT-base嵌入 + BiLSTM (作为特征提取器)\n",
    "3.  **模型三：** 微调BERT-base模型\n",
    "\n",
    "**评价指标：** 准确率（Accuracy）和宏平均F1值（Macro-F1）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境设置与依赖安装\n",
    "\n",
    "首先，我们安装必要的库。我们将需要 `torch`、用于BERT模型的 `transformers`、用于加载数据的 `pandas` 和 `pyarrow`，以及用于评估的 `scikit-learn`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理\n",
    "\n",
    "我们将从 `.parquet` 文件加载SNLI数据集。数据集包含句子对（前提和假设）和一个标签。\n",
    "\n",
    "**标签说明：**\n",
    "- `0`: 蕴含 (Entailment)\n",
    "- `1`: 中立 (Neutral)\n",
    "- `2`: 矛盾 (Contradiction)\n",
    "\n",
    "数据集中包含一些标签为-1的样本，表示标注者无法达成一致意见。我们将过滤掉这些样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: (9824, 3)\n",
      "验证集大小: (9842, 3)\n",
      "测试集大小: (9824, 3)\n",
      "\n",
      "数据样本示例:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>The church has cracks in the ceiling.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>The church is filled with song.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This church choir sings to the masses as they ...</td>\n",
       "      <td>A choir singing at a baseball game.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
       "      <td>The woman is young.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A woman with a green headscarf, blue shirt and...</td>\n",
       "      <td>The woman is very happy.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  This church choir sings to the masses as they ...   \n",
       "1  This church choir sings to the masses as they ...   \n",
       "2  This church choir sings to the masses as they ...   \n",
       "3  A woman with a green headscarf, blue shirt and...   \n",
       "4  A woman with a green headscarf, blue shirt and...   \n",
       "\n",
       "                              hypothesis  label  \n",
       "0  The church has cracks in the ceiling.      1  \n",
       "1        The church is filled with song.      0  \n",
       "2    A choir singing at a baseball game.      2  \n",
       "3                    The woman is young.      1  \n",
       "4               The woman is very happy.      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 定义文件路径\n",
    "train_file =r\"D:\\期末大作业\\期末大作业\\数据集\\SNLI\\test-00000-of-00001.parquet\"\n",
    "val_file = r\"D:\\期末大作业\\期末大作业\\数据集\\SNLI\\validation-00000-of-00001.parquet\"\n",
    "test_file = r\"D:\\期末大作业\\期末大作业\\数据集\\SNLI\\test-00000-of-00001.parquet\"\n",
    "\n",
    "# 加载数据集\n",
    "df_train = pd.read_parquet(train_file)\n",
    "df_val = pd.read_parquet(val_file)\n",
    "df_test = pd.read_parquet(test_file)\n",
    "\n",
    "# --- 数据清洗 ---\n",
    "# SNLI数据集使用-1作为标注者无法达成一致的标签，我们将移除这些样本。\n",
    "df_train = df_train[df_train['label'] != -1].reset_index(drop=True)\n",
    "df_val = df_val[df_val['label'] != -1].reset_index(drop=True)\n",
    "df_test = df_test[df_test['label'] != -1].reset_index(drop=True)\n",
    "\n",
    "print(\"训练集大小:\", df_train.shape)\n",
    "print(\"验证集大小:\", df_val.shape)\n",
    "print(\"测试集大小:\", df_test.shape)\n",
    "\n",
    "print(\"\\n数据样本示例:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型一：GloVe + BiLSTM\n",
    "\n",
    "该模型使用预训练的GloVe词向量作为BiLSTM网络的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe设置\n",
    "使用300维的向量 (`glove.800B.300d.txt`)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到GloVe文件: D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\n",
      "正在构建词汇表...\n",
      "正在加载GloVe词向量...\n",
      "已处理 100000 行...\n",
      "已处理 100000 行...\n",
      "已处理 200000 行...\n",
      "已处理 200000 行...\n",
      "已处理 300000 行...\n",
      "已处理 300000 行...\n",
      "已处理 400000 行...\n",
      "已处理 400000 行...\n",
      "已处理 500000 行...\n",
      "已处理 500000 行...\n",
      "已处理 600000 行...\n",
      "已处理 600000 行...\n",
      "已处理 700000 行...\n",
      "已处理 700000 行...\n",
      "已处理 800000 行...\n",
      "已处理 800000 行...\n",
      "已处理 900000 行...\n",
      "已处理 900000 行...\n",
      "已处理 1000000 行...\n",
      "已处理 1000000 行...\n",
      "已处理 1100000 行...\n",
      "已处理 1100000 行...\n",
      "已处理 1200000 行...\n",
      "已处理 1200000 行...\n",
      "已处理 1300000 行...\n",
      "已处理 1300000 行...\n",
      "已处理 1400000 行...\n",
      "已处理 1400000 行...\n",
      "已处理 1500000 行...\n",
      "已处理 1500000 行...\n",
      "已处理 1600000 行...\n",
      "已处理 1600000 行...\n",
      "已处理 1700000 行...\n",
      "已处理 1700000 行...\n",
      "已处理 1800000 行...\n",
      "已处理 1800000 行...\n",
      "已处理 1900000 行...\n",
      "已处理 1900000 行...\n",
      "已处理 2000000 行...\n",
      "已处理 2000000 行...\n",
      "已处理 2100000 行...\n",
      "已处理 2100000 行...\n",
      "GloVe文件共 2196017 行，成功加载 6371 个词向量 (96.69% 的词汇表)\n",
      "解析过程中遇到 0 个错误行\n",
      "\n",
      "数据准备完成。\n",
      "GloVe文件共 2196017 行，成功加载 6371 个词向量 (96.69% 的词汇表)\n",
      "解析过程中遇到 0 个错误行\n",
      "\n",
      "数据准备完成。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "# --- 参数配置 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "GLOVE_PATH = r\"D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\"\n",
    "# 检查GloVe文件是否存在\n",
    "if not os.path.exists(GLOVE_PATH):\n",
    "    raise FileNotFoundError(f\"GloVe文件不存在于: {GLOVE_PATH}\")\n",
    "else:\n",
    "    print(f\"找到GloVe文件: {GLOVE_PATH}\")\n",
    "\n",
    "# --- 文本预处理与词汇表构建 ---\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9' ]+\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "print(\"正在构建词汇表...\")\n",
    "word_counts = Counter()\n",
    "for text in pd.concat([df_train['premise'], df_train['hypothesis']]):\n",
    "    word_counts.update(tokenizer(text))\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_idx = {word: i+2 for i, word in enumerate(vocab)} # i+2 是为了给 <pad> 和 <unk> 留出位置\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<unk>'] = 1\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "\n",
    "# --- GloVe 词向量矩阵 ---\n",
    "print(\"正在加载GloVe词向量...\")\n",
    "glove_embeddings = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "word_found = 0\n",
    "total_lines = 0\n",
    "error_count = 0\n",
    "\n",
    "with open(GLOVE_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        total_lines += 1\n",
    "        if line_num % 100000 == 0:\n",
    "            print(f\"已处理 {line_num} 行...\")\n",
    "        \n",
    "        try:\n",
    "            parts = line.strip().split(' ')\n",
    "            word = parts[0]\n",
    "            \n",
    "            if word in word_to_idx:\n",
    "                # 确保有足够的值来创建向量\n",
    "                if len(parts) >= EMBEDDING_DIM + 1:  # 单词 + 300维向量\n",
    "                    vector = np.array(parts[1:EMBEDDING_DIM+1], dtype=np.float32)\n",
    "                    if len(vector) == EMBEDDING_DIM:\n",
    "                        glove_embeddings[word_to_idx[word]] = vector\n",
    "                        word_found += 1\n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            if error_count < 10:  # 只打印前几个错误\n",
    "                print(f\"第 {line_num} 行解析错误: {e}\")\n",
    "                print(f\"问题行内容: {line[:50]}...\" if len(line) > 50 else line)\n",
    "            continue\n",
    "\n",
    "print(f\"GloVe文件共 {total_lines} 行，成功加载 {word_found} 个词向量 ({word_found/VOCAB_SIZE*100:.2f}% 的词汇表)\")\n",
    "print(f\"解析过程中遇到 {error_count} 个错误行\")\n",
    "\n",
    "glove_embeddings = torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "\n",
    "# --- PyTorch 数据集 ---\n",
    "class SNLIDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_to_idx, max_len=50):\n",
    "        self.df = dataframe\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        premise = self.df.loc[idx, 'premise']\n",
    "        hypothesis = self.df.loc[idx, 'hypothesis']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        premise_tokens = [self.word_to_idx.get(word, self.word_to_idx['<unk>']) for word in tokenizer(premise)]\n",
    "        hypothesis_tokens = [self.word_to_idx.get(word, self.word_to_idx['<unk>']) for word in tokenizer(hypothesis)]\n",
    "\n",
    "        # 使用简单拼接（可以改进，例如使用[SEP]标记）\n",
    "        tokens = premise_tokens + hypothesis_tokens\n",
    "        \n",
    "        # 填充/截断\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens.extend([self.word_to_idx['<pad>']] * (self.max_len - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(tokens), torch.tensor(label)\n",
    "\n",
    "# --- 创建数据加载器 ---\n",
    "train_dataset = SNLIDataset(df_train, word_to_idx)\n",
    "val_dataset = SNLIDataset(df_val, word_to_idx)\n",
    "test_dataset = SNLIDataset(df_test, word_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\n数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BiLSTM 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (embedding): Embedding(6589, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False # 冻结词向量，不参与训练\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                              hidden_dim, \n",
    "                              num_layers=n_layers, \n",
    "                              bidirectional=True, \n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # *2因为是双向LSTM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # _ 是所有时间步的输出, (hidden, cell) 是最后一个时间步的隐藏状态和细胞状态\n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 拼接前向和后向的最终隐藏状态\n",
    "        # hidden 的形状是 [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 实例化模型\n",
    "model1 = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT, glove_embeddings).to(DEVICE)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 训练与评估循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型一 (GloVe + BiLSTM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 01 | 训练损失: 1.102 | 训练准确率: 32.68% | 验证损失: 1.101 | 验证准确率: 33.58% | 验证F1: 0.269\n",
      "轮次: 02 | 训练损失: 1.100 | 训练准确率: 32.81% | 验证损失: 1.099 | 验证准确率: 33.10% | 验证F1: 0.261\n",
      "轮次: 02 | 训练损失: 1.100 | 训练准确率: 32.81% | 验证损失: 1.099 | 验证准确率: 33.10% | 验证F1: 0.261\n",
      "轮次: 03 | 训练损失: 1.100 | 训练准确率: 33.75% | 验证损失: 1.099 | 验证准确率: 33.82% | 验证F1: 0.169\n",
      "轮次: 03 | 训练损失: 1.100 | 训练准确率: 33.75% | 验证损失: 1.099 | 验证准确率: 33.82% | 验证F1: 0.169\n",
      "轮次: 04 | 训练损失: 1.099 | 训练准确率: 33.47% | 验证损失: 1.099 | 验证准确率: 33.15% | 验证F1: 0.314\n",
      "轮次: 04 | 训练损失: 1.099 | 训练准确率: 33.47% | 验证损失: 1.099 | 验证准确率: 33.15% | 验证F1: 0.314\n",
      "轮次: 05 | 训练损失: 1.099 | 训练准确率: 33.38% | 验证损失: 1.099 | 验证准确率: 33.11% | 验证F1: 0.247\n",
      "轮次: 05 | 训练损失: 1.099 | 训练准确率: 33.38% | 验证损失: 1.099 | 验证准确率: 33.11% | 验证F1: 0.247\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = accuracy_score(labels.cpu(), predictions.argmax(1).cpu())\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_model(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss / len(iterator), acc, f1\n",
    "\n",
    "# --- 训练模型一 ---\n",
    "print(\"开始训练模型一 (GloVe + BiLSTM)...\")\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_model(model1, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate_model(model1, val_loader, criterion)\n",
    "    \n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 训练准确率: {train_acc*100:.2f}% | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. 在测试集上对模型一进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型一 测试集结果 -> 准确率: 33.83% | Macro-F1: 0.255\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = evaluate_model(model1, test_loader, criterion)\n",
    "print(f'模型一 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f}')\n",
    "results = {}\n",
    "results['模型一 (GloVe + BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型二：BERT嵌入 + BiLSTM\n",
    "\n",
    "在这个模型中，我们使用预训练的BERT模型 (`bert-base-uncased`) 作为特征提取器。它的权重被冻结，输出的嵌入向量被送入一个BiLSTM网络，类似于模型一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用于BERT的数据准备完成。\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- 参数配置 ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN_BERT = 128 # BERT最大长度是512\n",
    "\n",
    "# --- BERT 分词器 ---\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# --- 用于BERT的PyTorch数据集 ---\n",
    "class SNLIDatasetBERT(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        premise = self.df.loc[idx, 'premise']\n",
    "        hypothesis = self.df.loc[idx, 'hypothesis']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            premise,\n",
    "            hypothesis,\n",
    "            add_special_tokens=True, # 添加 '[CLS]' 和 '[SEP]' 特殊符号\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt', # 返回PyTorch张量\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 创建数据加载器 ---\n",
    "train_dataset_bert = SNLIDatasetBERT(df_train, tokenizer_bert, MAX_LEN_BERT)\n",
    "val_dataset_bert = SNLIDatasetBERT(df_val, tokenizer_bert, MAX_LEN_BERT)\n",
    "test_dataset_bert = SNLIDatasetBERT(df_test, tokenizer_bert, MAX_LEN_BERT)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=32, shuffle=True) # BERT模型需要更小的批量大小\n",
    "val_loader_bert = DataLoader(val_dataset_bert, batch_size=32)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=32)\n",
    "print(\"用于BERT的数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. BERT+BiLSTM 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertBiLSTMClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(768, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size'] # BERT的嵌入维度\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                              hidden_dim,\n",
    "                              num_layers=n_layers,\n",
    "                              bidirectional=True,\n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 不计算梯度，以冻结BERT\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "bert_model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# 冻结BERT的参数\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 实例化模型\n",
    "model2 = BertBiLSTMClassifier(bert_model, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 训练与评估（模型二）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练模型二 (BERT + BiLSTM)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次: 01 | 训练损失: 1.041 | 验证损失: 0.979 | 验证准确率: 51.28% | 验证F1: 0.506\n",
      "轮次: 02 | 训练损失: 0.963 | 验证损失: 0.896 | 验证准确率: 58.97% | 验证F1: 0.586\n",
      "轮次: 02 | 训练损失: 0.963 | 验证损失: 0.896 | 验证准确率: 58.97% | 验证F1: 0.586\n",
      "轮次: 03 | 训练损失: 0.922 | 验证损失: 0.854 | 验证准确率: 61.66% | 验证F1: 0.611\n",
      "轮次: 03 | 训练损失: 0.922 | 验证损失: 0.854 | 验证准确率: 61.66% | 验证F1: 0.611\n"
     ]
    }
   ],
   "source": [
    "def train_bert_bilstm(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_bilstm(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss / len(iterator), acc, f1\n",
    "\n",
    "# --- 训练模型二 ---\n",
    "# 注意：我们只将未冻结层的参数传递给优化器\n",
    "print(\"\\n开始训练模型二 (BERT + BiLSTM)...\")\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 由于模型较大，减少训练轮数\n",
    "for epoch in range(3):\n",
    "    train_loss = train_bert_bilstm(model2, train_loader_bert, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate_bert_bilstm(model2, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 在测试集上对模型二进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型二 测试集结果 -> 准确率: 63.20% | Macro-F1: 0.626\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = evaluate_bert_bilstm(model2, test_loader_bert, criterion)\n",
    "print(f'模型二 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f}')\n",
    "results['模型二 (BERT嵌入 + BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型三：微调BERT\n",
    "\n",
    "这是最常用且最强大的方法。我们采用一个带分类头的预训练BERT模型 (`BertForSequenceClassification`)，并在我们的特定任务上对整个模型进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --- 加载模型 ---\u001b[39;00m\n\u001b[32m      4\u001b[39m model3 = BertForSequenceClassification.from_pretrained(\n\u001b[32m      5\u001b[39m     BERT_MODEL_NAME,\n\u001b[32m      6\u001b[39m     num_labels=NUM_CLASSES,\n\u001b[32m      7\u001b[39m     output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      8\u001b[39m     output_hidden_states=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      9\u001b[39m ).to(DEVICE)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AdamW' from 'transformers' (e:\\anaconda3\\envs\\pytorch2.3.0\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW  # 从PyTorch导入AdamW而不是transformers\n",
    "\n",
    "# --- 加载模型 ---\n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    BERT_MODEL_NAME,\n",
    "    num_labels=NUM_CLASSES,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ").to(DEVICE)\n",
    "\n",
    "# --- 优化器与学习率调度器 ---\n",
    "optimizer = AdamW(model3.parameters(), lr=2e-5, eps=1e-8)\n",
    "EPOCHS_BERT_FINETUNE = 3\n",
    "total_steps = len(train_loader_bert) * EPOCHS_BERT_FINETUNE\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "print(\"BERT微调模型已加载。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 训练与评估（模型三）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_finetune(model, iterator, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 直接将labels传入，模型会自动计算损失\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 防止梯度爆炸\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_finetune(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return epoch_loss / len(iterator), acc, f1\n",
    "\n",
    "# --- 训练模型三 ---\n",
    "print(\"\\n开始训练模型三 (微调BERT)...\")\n",
    "for epoch in range(EPOCHS_BERT_FINETUNE): # 微调通常需要较少的训练轮数\n",
    "    train_loss = train_bert_finetune(model3, train_loader_bert, optimizer, scheduler, criterion)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate_bert_finetune(model3, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 在测试集上对模型三进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_f1 = evaluate_bert_finetune(model3, test_loader_bert, criterion)\n",
    "print(f'模型三 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f}')\n",
    "results['模型三 (微调BERT)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结与性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results).T\n",
    "df_results['Accuracy'] = df_results['Accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "df_results['Macro-F1'] = df_results['Macro-F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"--- SNLI测试集最终性能对比 ---\")\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验分析\n",
    "\n",
    "1.  **模型一 (GloVe + BiLSTM):** 该模型是一个强大的基线模型。GloVe词向量能够捕捉单词间的语义关系，而BiLSTM则从序列中学习上下文信息。然而，它的理解能力是有限的，因为GloVe词向量是静态的——它们不会根据句子的具体上下文而改变。\n",
    "\n",
    "2.  **模型二 (BERT嵌入 + BiLSTM):** 该模型在模型一的基础上进行了改进，使用了来自BERT的上下文相关词向量。对于同一个词，BERT会根据其周围的词生成不同的向量。将BERT用作固定的特征提取器比微调整个模型的计算成本要低。顶部的BiLSTM有助于聚合这些上下文特征以用于最终的分类决策。我们通常会看到比GloVe有性能上的提升。\n",
    "\n",
    "3.  **模型三 (微调BERT):** 这通常是性能最好的模型。通过微调整个BERT模型，我们使其内部的权重（特别是注意力机制）能够专门适应SNLI任务的细微差别。模型不仅学习如何表示词语，而且学习如何直接执行推理任务。`[CLS]`标记的最终隐藏状态被设计为整个输入序列的表示，使其非常适合分类任务。这种端到端的训练方式几乎总能在像NLI这样的句子对分类任务上产生最佳结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
