{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务一：IMDB-10 情感分类（基于SNLI模板修改）\n",
    "\n",
    "本Notebook为IMDB电影评论的十分类情感（评分1-10）任务，基于SNLI代码模板修改而来。\n",
    "\n",
    "三个模型分别是：\n",
    "1.  **模型一：** GloVe嵌入 + BiLSTM\n",
    "2.  **模型二：** BERT-base嵌入 + BiLSTM (作为特征提取器)\n",
    "3.  **模型三：** 微调BERT-base模型\n",
    "\n",
    "**评价指标：** 准确率（Accuracy）、宏平均F1值（Macro-F1）和均方根误差（RMSE）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境设置与依赖安装\n",
    "\n",
    "首先，我们安装必要的库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据加载与预处理\n",
    "\n",
    "我们将从 `.txt.ss` 文件加载IMDB数据集。数据集包含电影评论文本和评分标签。\n",
    "\n",
    "**标签说明：**\n",
    "- `0-9`: 评分1-10转换后的标签\n",
    "\n",
    "我们需要将评分从1-10转换为0-9以便模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_imdb_data(file_path):\n",
    "    \"\"\"加载并解析IMDB数据文件，使用正则表达式查找标签。\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                match = re.search(r'\\t\\t(\\d+)\\t\\t', line)\n",
    "                if match:\n",
    "                    rating = int(match.group(1))\n",
    "                    text_start_index = match.end()\n",
    "                    text = line[text_start_index:].strip()\n",
    "                    text = text.replace('<sssss>', ' ').strip()\n",
    "                    labels.append(rating - 1)  # 转换为0-9\n",
    "                    texts.append(text)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {file_path}。请确保数据文件在当前目录下。\")\n",
    "        return pd.DataFrame({'text': [], 'label': []})\n",
    "    \n",
    "    df = pd.DataFrame({'text': texts, 'label': labels})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# 定义文件路径\n",
    "train_file = 'imdb.train.txt.ss'\n",
    "dev_file = 'imdb.dev.txt.ss'\n",
    "test_file = 'imdb.test.txt.ss'\n",
    "\n",
    "# 加载所有数据集\n",
    "df_train = load_imdb_data(train_file)\n",
    "df_val = load_imdb_data(dev_file)\n",
    "df_test = load_imdb_data(test_file)\n",
    "\n",
    "if not df_train.empty:\n",
    "    print(f\"训练集大小: {df_train.shape}\")\n",
    "    print(f\"验证集大小: {df_val.shape}\")\n",
    "    print(f\"测试集大小: {df_test.shape}\")\n",
    "    print(\"\\n数据样本示例:\")\n",
    "    print(df_train.head())\n",
    "    \n",
    "    # 显示标签分布\n",
    "    print(\"\\n训练集标签分布:\")\n",
    "    print(df_train['label'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"数据加载失败，请检查文件路径！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型一：GloVe + BiLSTM\n",
    "\n",
    "该模型使用预训练的GloVe词向量作为BiLSTM网络的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. GloVe设置\n",
    "使用300维的向量。可以选择使用glove.6B.300d.txt（较小）或glove.840B.300d.txt（较大）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 参数配置 ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_CLASSES = 10  # IMDB 10分类\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "EPOCHS = 5\n",
    "MAX_LEN = 512  # 文本最大长度\n",
    "\n",
    "# GloVe路径 - 您可以选择使用6B或840B版本\n",
    "GLOVE_PATH = r\"D:\\glove_vectors\\glove.6B.300d.txt\"  # 或者使用840B版本\n",
    "# GLOVE_PATH = r\"D:\\glove_vectors\\glove.840B.300d\\glove.840B.300d.txt\"\n",
    "\n",
    "# 检查GloVe文件是否存在\n",
    "if not os.path.exists(GLOVE_PATH):\n",
    "    print(f\"警告: GloVe文件不存在于: {GLOVE_PATH}\")\n",
    "    print(\"将使用随机初始化的词向量\")\n",
    "    USE_GLOVE = False\n",
    "else:\n",
    "    print(f\"找到GloVe文件: {GLOVE_PATH}\")\n",
    "    USE_GLOVE = True\n",
    "\n",
    "# --- 文本预处理与词汇表构建 ---\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9' ]+\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "print(\"正在构建词汇表...\")\n",
    "word_counts = Counter()\n",
    "for text in df_train['text']:\n",
    "    word_counts.update(tokenizer(text))\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "word_to_idx = {word: i+2 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<pad>'] = 0\n",
    "word_to_idx['<unk>'] = 1\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "\n",
    "print(f\"词汇表大小: {VOCAB_SIZE}\")\n",
    "\n",
    "# --- GloVe 词向量矩阵 ---\n",
    "print(\"正在加载GloVe词向量...\")\n",
    "glove_embeddings = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "word_found = 0\n",
    "\n",
    "if USE_GLOVE:\n",
    "    with open(GLOVE_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line_num % 50000 == 0:\n",
    "                print(f\"已处理 {line_num} 行...\")\n",
    "            \n",
    "            try:\n",
    "                parts = line.strip().split(' ')\n",
    "                word = parts[0]\n",
    "                \n",
    "                if word in word_to_idx:\n",
    "                    if len(parts) >= EMBEDDING_DIM + 1:\n",
    "                        vector = np.array(parts[1:EMBEDDING_DIM+1], dtype=np.float32)\n",
    "                        if len(vector) == EMBEDDING_DIM:\n",
    "                            glove_embeddings[word_to_idx[word]] = vector\n",
    "                            word_found += 1\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"成功加载 {word_found} 个词向量 ({word_found/VOCAB_SIZE*100:.2f}% 的词汇表)\")\n",
    "else:\n",
    "    print(\"使用随机初始化的词向量\")\n",
    "    glove_embeddings = np.random.normal(0, 0.1, (VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "glove_embeddings = torch.tensor(glove_embeddings, dtype=torch.float32)\n",
    "\n",
    "# --- PyTorch 数据集 ---\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_to_idx, max_len):\n",
    "        self.df = dataframe\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        tokens = [self.word_to_idx.get(word, self.word_to_idx['<unk>']) for word in tokenizer(text)]\n",
    "        \n",
    "        # 填充/截断\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens.extend([self.word_to_idx['<pad>']] * (self.max_len - len(tokens)))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(tokens), torch.tensor(label)\n",
    "\n",
    "# --- 创建数据加载器 ---\n",
    "train_dataset = IMDBDataset(df_train, word_to_idx, MAX_LEN)\n",
    "val_dataset = IMDBDataset(df_val, word_to_idx, MAX_LEN)\n",
    "test_dataset = IMDBDataset(df_test, word_to_idx, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\n数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BiLSTM 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False  # 冻结词向量\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                              hidden_dim, \n",
    "                              num_layers=n_layers, \n",
    "                              bidirectional=True, \n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2因为是双向LSTM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # 拼接前向和后向的最终隐藏状态\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 实例化模型\n",
    "model1 = BiLSTMClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT, glove_embeddings).to(DEVICE)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 训练与评估循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # 计算准确率\n",
    "        acc = accuracy_score(labels.cpu(), predictions.argmax(1).cpu())\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_model(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # 计算RMSE（将标签转回1-10）\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型一 ---\n",
    "print(\"开始训练模型一 (GloVe + BiLSTM)...\")\n",
    "optimizer = optim.Adam(model1.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_model(model1, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_model(model1, val_loader, criterion)\n",
    "    \n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 训练准确率: {train_acc*100:.2f}% | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. 在测试集上对模型一进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_f1, test_rmse = evaluate_model(model1, test_loader, criterion)\n",
    "print(f'模型一 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results = {}\n",
    "results['模型一 (GloVe + BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型二：BERT嵌入 + BiLSTM\n",
    "\n",
    "在这个模型中，我们使用预训练的BERT模型作为特征提取器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# --- 参数配置 ---\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN_BERT = 256  # 减少到256以节省显存\n",
    "BATCH_SIZE_BERT = 32  # 较小的批量大小\n",
    "\n",
    "# --- BERT 分词器 ---\n",
    "local_bert_path = 'D:/models/bert-base-uncased'\n",
    "\n",
    "if os.path.exists(local_bert_path):\n",
    "    print(f\"从本地路径加载BERT分词器: {local_bert_path}\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(local_bert_path)\n",
    "else:\n",
    "    print(f\"从网络下载BERT分词器: {BERT_MODEL_NAME}\")\n",
    "    tokenizer_bert = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# --- 用于BERT的PyTorch数据集 ---\n",
    "class IMDBDatasetBERT(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# --- 创建数据加载器 ---\n",
    "train_dataset_bert = IMDBDatasetBERT(df_train, tokenizer_bert, MAX_LEN_BERT)\n",
    "val_dataset_bert = IMDBDatasetBERT(df_val, tokenizer_bert, MAX_LEN_BERT)\n",
    "test_dataset_bert = IMDBDatasetBERT(df_test, tokenizer_bert, MAX_LEN_BERT)\n",
    "\n",
    "train_loader_bert = DataLoader(train_dataset_bert, batch_size=BATCH_SIZE_BERT, shuffle=True)\n",
    "val_loader_bert = DataLoader(val_dataset_bert, batch_size=BATCH_SIZE_BERT)\n",
    "test_loader_bert = DataLoader(test_dataset_bert, batch_size=BATCH_SIZE_BERT)\n",
    "print(\"用于BERT的数据准备完成。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. BERT+BiLSTM 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                              hidden_dim,\n",
    "                              num_layers=n_layers,\n",
    "                              bidirectional=True,\n",
    "                              dropout=dropout if n_layers > 1 else 0,\n",
    "                              batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # 冻结BERT，不计算梯度\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# 加载预训练的BERT模型\n",
    "if os.path.exists(local_bert_path):\n",
    "    bert_model = BertModel.from_pretrained(local_bert_path)\n",
    "else:\n",
    "    bert_model = BertModel.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "# 冻结BERT的参数\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 实例化模型\n",
    "model2 = BertBiLSTMClassifier(bert_model, HIDDEN_DIM, NUM_CLASSES, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "print(\"模型二已创建\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 训练与评估（模型二）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_bilstm(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_bilstm(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(predictions.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型二 ---\n",
    "print(\"\\n开始训练模型二 (BERT + BiLSTM)...\")\n",
    "optimizer = optim.Adam(model2.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 减少训练轮数\n",
    "for epoch in range(3):\n",
    "    train_loss = train_bert_bilstm(model2, train_loader_bert, optimizer, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_bert_bilstm(model2, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. 在测试集上对模型二进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_f1, test_rmse = evaluate_bert_bilstm(model2, test_loader_bert, criterion)\n",
    "print(f'模型二 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型二 (BERT嵌入 + BiLSTM)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型三：微调BERT\n",
    "\n",
    "这是最常用且最强大的方法。我们采用一个带分类头的预训练BERT模型，并在我们的特定任务上对整个模型进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# --- 加载模型 ---\n",
    "if os.path.exists(local_bert_path):\n",
    "    model3 = BertForSequenceClassification.from_pretrained(\n",
    "        local_bert_path,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ).to(DEVICE)\n",
    "else:\n",
    "    model3 = BertForSequenceClassification.from_pretrained(\n",
    "        BERT_MODEL_NAME,\n",
    "        num_labels=NUM_CLASSES,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "# --- 优化器与学习率调度器 ---\n",
    "optimizer = AdamW(model3.parameters(), lr=2e-5, eps=1e-8)\n",
    "EPOCHS_BERT_FINETUNE = 3\n",
    "total_steps = len(train_loader_bert) * EPOCHS_BERT_FINETUNE\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "print(\"BERT微调模型已加载。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 训练与评估（模型三）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_finetune(model, iterator, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_bert_finetune(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # 计算RMSE\n",
    "    preds_rmse = np.array(all_preds) + 1\n",
    "    labels_rmse = np.array(all_labels) + 1\n",
    "    rmse = np.sqrt(mean_squared_error(labels_rmse, preds_rmse))\n",
    "    \n",
    "    return epoch_loss / len(iterator), acc, f1, rmse\n",
    "\n",
    "# --- 训练模型三 ---\n",
    "print(\"\\n开始训练模型三 (微调BERT)...\")\n",
    "for epoch in range(EPOCHS_BERT_FINETUNE):\n",
    "    train_loss = train_bert_finetune(model3, train_loader_bert, optimizer, scheduler, criterion)\n",
    "    valid_loss, valid_acc, valid_f1, valid_rmse = evaluate_bert_finetune(model3, val_loader_bert, criterion)\n",
    "    print(f'轮次: {epoch+1:02} | 训练损失: {train_loss:.3f} | 验证损失: {valid_loss:.3f} | 验证准确率: {valid_acc*100:.2f}% | 验证F1: {valid_f1:.3f} | 验证RMSE: {valid_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. 在测试集上对模型三进行最终评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_f1, test_rmse = evaluate_bert_finetune(model3, test_loader_bert, criterion)\n",
    "print(f'模型三 测试集结果 -> 准确率: {test_acc*100:.2f}% | Macro-F1: {test_f1:.3f} | RMSE: {test_rmse:.3f}')\n",
    "results['模型三 (微调BERT)'] = {'Accuracy': test_acc, 'Macro-F1': test_f1, 'RMSE': test_rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结与性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建结果DataFrame\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results['Accuracy'] = df_results['Accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "df_results['Macro-F1'] = df_results['Macro-F1'].apply(lambda x: f\"{x:.4f}\")\n",
    "df_results['RMSE'] = df_results['RMSE'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"--- IMDB-10测试集最终性能对比 ---\")\n",
    "print(df_results)\n",
    "\n",
    "# 保存结果\n",
    "df_results.to_csv('imdb_results.csv')\n",
    "print(\"\\n结果已保存到 imdb_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验分析\n",
    "\n",
    "1. **模型一 (GloVe + BiLSTM)**: 使用静态词向量的基线模型，性能受限于词向量的表达能力。\n",
    "\n",
    "2. **模型二 (BERT嵌入 + BiLSTM)**: 使用BERT作为特征提取器，应该比模型一有显著提升。\n",
    "\n",
    "3. **模型三 (微调BERT)**: 端到端微调的BERT模型，通常具有最佳性能。\n",
    "\n",
    "**关键改进点**:\n",
    "- 将BERT输入长度减少到256以节省显存\n",
    "- 添加了RMSE评估指标\n",
    "- 支持本地BERT模型路径\n",
    "- 增加了错误处理和进度提示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.3.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
